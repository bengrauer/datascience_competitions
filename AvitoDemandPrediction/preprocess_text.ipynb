{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================================================\n",
    "# Author: Ben Grauer\n",
    "# Purpose: Script to read in and gather metrics on the item description text\n",
    "#\n",
    "#====================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# import spacy\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "from unidecode import unidecode\n",
    "import codecs\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_additional_columns_to_dataframe(df, columnList):\n",
    "    \n",
    "    # Create the translated dataframe column\n",
    "    for i in columnList:\n",
    "        # check for existing\n",
    "        if i in df:\n",
    "            print(str(i) + ' already exists.')\n",
    "        else:\n",
    "            df[i] = ''\n",
    "            \n",
    "    if 'text_description_processed' in df:\n",
    "        print('Existing records processed')\n",
    "        \n",
    "    else:\n",
    "        df['text_description_processed'] = 0\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnList = [\\\n",
    "'desc_numWords',\\\n",
    "'desc_numStopWords',\\\n",
    "\n",
    "'desc_numNouns',\\\n",
    "'desc_numVerbs',\\\n",
    "'desc_numAdjs',\\\n",
    "'desc_numSymbols',\\\n",
    "'desc_wordsCondition',\\\n",
    "'desc_wordBargainOrDeal',\\\n",
    "\n",
    "'desc_numNonASCIIWords',\\\n",
    "'desc_numNumericWords',\\\n",
    "'desc_numUpperCaseWords',\\\n",
    "\n",
    "'desc_avgImportantWordLength',\\\n",
    "'desc_avgAllWordLength',\\\n",
    "              \n",
    "'desc_numSentences',\\\n",
    "'desc_avgWordsPerSentence',\\\n",
    "'desc_avgWordLengthPerSentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['desc_numWords',\n",
       " 'desc_numStopWords',\n",
       " 'desc_numNouns',\n",
       " 'desc_numVerbs',\n",
       " 'desc_numAdjs',\n",
       " 'desc_numSymbols',\n",
       " 'desc_wordsCondition',\n",
       " 'desc_wordBargainOrDeal',\n",
       " 'desc_numNonASCIIWords',\n",
       " 'desc_numNumericWords',\n",
       " 'desc_numUpperCaseWords',\n",
       " 'desc_avgImportantWordLength',\n",
       " 'desc_avgAllWordLength',\n",
       " 'desc_numSentences',\n",
       " 'desc_avgWordsPerSentence',\n",
       " 'desc_avgWordLengthPerSentence']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy for English (was translated from Russian)\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Constant for group number.  Ended up not needing to run in parallel as this process ran within 8 hrs\n",
    "CONST_GROUP_NUM = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file not found - reading raw file\n"
     ]
    }
   ],
   "source": [
    "groupedFileName = 'D:/project/data/kg_avito_demand/FullTextProcessedTrain_' + str(CONST_GROUP_NUM) + '.csv'\n",
    "\n",
    "# Check to see if group file exists or not\n",
    "if os.path.isfile(groupedFileName) == False:\n",
    "    print('file not found - reading raw file')\n",
    "    # Create the dataframe from the base file\n",
    "    dfGroup = pd.read_csv('D:/project/data/kg_avito_demand/train.csv')\n",
    "    \n",
    "    # append the additional columns\n",
    "    dfGroup = add_additional_columns_to_dataframe(dfGroup, columnList)\n",
    "    \n",
    "    # else read it in, and re-write to it\n",
    "    df = dfGroup.copy()\n",
    "    \n",
    "else:\n",
    "    print('file found - read in')\n",
    "    dfGroup = pd.read_csv(groupedFileName)\n",
    "    df = dfGroup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If they did not translate, then fill with blank space\n",
    "df['description_translated'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total row to convert: 1503424 out of: 1503424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:1128: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row (2018-06-03 15:22:48.879128): 50000 and creating file snapshot. 0:13:08.048821\n",
      "Processed row (2018-06-03 15:36:34.683818): 100000 and creating file snapshot. 0:26:53.853511\n",
      "Processed row (2018-06-03 15:50:31.662038): 150000 and creating file snapshot. 0:40:50.831731\n",
      "Processed row (2018-06-03 16:05:43.909717): 200000 and creating file snapshot. 0:56:03.079410\n",
      "Processed row (2018-06-03 16:20:56.015418): 250000 and creating file snapshot. 1:11:15.185111\n",
      "Processed row (2018-06-03 16:36:10.031172): 300000 and creating file snapshot. 1:26:29.200865\n",
      "Processed row (2018-06-03 16:50:56.938548): 350000 and creating file snapshot. 1:41:16.108241\n",
      "Processed row (2018-06-03 17:05:26.605513): 400000 and creating file snapshot. 1:55:45.775206\n",
      "Processed row (2018-06-03 17:19:48.690230): 450000 and creating file snapshot. 2:10:07.859923\n",
      "Processed row (2018-06-03 17:33:44.514543): 500000 and creating file snapshot. 2:24:03.684236\n",
      "Processed row (2018-06-03 17:45:22.627319): 550000 and creating file snapshot. 2:35:41.797012\n",
      "Processed row (2018-06-03 17:56:56.768714): 600000 and creating file snapshot. 2:47:15.938407\n",
      "Processed row (2018-06-03 18:08:29.720294): 650000 and creating file snapshot. 2:58:48.889987\n",
      "Processed row (2018-06-03 18:20:07.317447): 700000 and creating file snapshot. 3:10:26.487140\n",
      "Processed row (2018-06-03 18:31:47.560644): 750000 and creating file snapshot. 3:22:06.730337\n",
      "Processed row (2018-06-03 18:43:25.939707): 800000 and creating file snapshot. 3:33:45.109400\n",
      "Processed row (2018-06-03 18:55:03.326423): 850000 and creating file snapshot. 3:45:22.496116\n",
      "Processed row (2018-06-03 19:06:45.497373): 900000 and creating file snapshot. 3:57:04.667066\n",
      "Processed row (2018-06-03 19:18:28.852127): 950000 and creating file snapshot. 4:08:48.021820\n",
      "Processed row (2018-06-03 19:30:13.102486): 1000000 and creating file snapshot. 4:20:32.272179\n",
      "Processed row (2018-06-03 19:41:53.112700): 1050000 and creating file snapshot. 4:32:12.282393\n",
      "Processed row (2018-06-03 19:53:35.879027): 1100000 and creating file snapshot. 4:43:55.048720\n",
      "Processed row (2018-06-03 20:05:16.254749): 1150000 and creating file snapshot. 4:55:35.424442\n",
      "Processed row (2018-06-03 20:16:57.680663): 1200000 and creating file snapshot. 5:07:16.850356\n",
      "Processed row (2018-06-03 20:28:47.455245): 1250000 and creating file snapshot. 5:19:06.624938\n",
      "Processed row (2018-06-03 20:40:34.905047): 1300000 and creating file snapshot. 5:30:54.074740\n",
      "Processed row (2018-06-03 20:52:24.710547): 1350000 and creating file snapshot. 5:42:43.880240\n",
      "Processed row (2018-06-03 21:04:13.461867): 1400000 and creating file snapshot. 5:54:32.631560\n",
      "Processed row (2018-06-03 21:16:11.232417): 1450000 and creating file snapshot. 6:06:30.402110\n",
      "Processed row (2018-06-03 21:28:00.471650): 1500000 and creating file snapshot. 6:18:19.641343\n",
      "Finished: 6:19:49.123606 - Now Writing final file ...\n",
      "Finished Writing File\n"
     ]
    }
   ],
   "source": [
    "print('Total row to convert: ' + str(len(df[df['text_description_processed']==0])) + ' out of: ' + str(len(df)))\n",
    "startTime = datetime.now()\n",
    "print(str(datetime.now())) # 06/03/2018 - 3:15 pm\n",
    "\n",
    "# For each row in the data set\n",
    "for i, row in df.iterrows():\n",
    "\n",
    "    # Here skip if we have already processed (or only process if still outstanding)\n",
    "    if df.at[i, 'text_description_processed'] == 0:\n",
    "\n",
    "        # Handle for nan\n",
    "        if df.at[i, 'description_translated'] == np.nan:\n",
    "            df.at[i, 'description_translated'] = ''\n",
    "        \n",
    "        # START STATS\n",
    "        # Assign the document\n",
    "        doc = nlp(df.at[i, 'description_translated'])\n",
    "        \n",
    "        # Words / Tokens\n",
    "        numWords = 0\n",
    "        numNonASCIIWords = 0\n",
    "        numNumericWords = 0\n",
    "        numUpperCaseWords = 0\n",
    "        numStopWords = 0\n",
    "\n",
    "        numNouns = 0\n",
    "        numVerbs = 0\n",
    "        numAdjs = 0\n",
    "\n",
    "        # Other Items\n",
    "        numSymbols = 0\n",
    "        wordsCondition = ''\n",
    "        wordBargainOrDeal = ''  # not sure about this one, will think about it\n",
    "\n",
    "        # Word Avg Lengths\n",
    "        allWordLengths = []\n",
    "        importantWordLengths = []\n",
    "\n",
    "        # Sentences\n",
    "        sentenceCount = 0\n",
    "        avgWordsPerSentence = 0\n",
    "        avgWordLengthPerSentence = 0\n",
    "        numWordsPerSentence = 0\n",
    "\n",
    "        # TOKENS\n",
    "        # BEGIN - for token in doc:\n",
    "        for docIndex in range(len(doc)):\n",
    "            # Assign the token/word\n",
    "            token = doc[docIndex]\n",
    "            \n",
    "            # Skip all stop words / Punctuation\n",
    "            if token.is_stop == False and token.pos_!='PUNCT':\n",
    "\n",
    "                # Number of real words\n",
    "                if token.is_ascii == True:\n",
    "                    numWords = numWords + 1\n",
    "\n",
    "                ## Nouns\n",
    "                if token.pos_ == 'NOUN':\n",
    "                    numNouns = numNouns+1\n",
    "\n",
    "                ## Verbs\n",
    "                if token.pos_ == 'VERB':\n",
    "                    numVerbs = numVerbs+1\n",
    "\n",
    "                ## Adjectives\n",
    "                if token.pos_ == 'ADJ':\n",
    "                    numAdjs = numAdjs + 1\n",
    "\n",
    "                ## OTHER DESCRIBING FACTORS ##\n",
    "                # Find the number of ALL upper case words\n",
    "                if token.is_upper == True:\n",
    "                    numUpperCaseWords = numUpperCaseWords + 1\n",
    "\n",
    "                # Check for a type of condition - good, great, etc\n",
    "                if token.text.upper() == \"CONDITION\":\n",
    "                    if docIndex-1 >= 0:\n",
    "                        if (doc[docIndex-1].pos_ == 'ADJ'):\n",
    "                            wordsCondition = str(doc[docIndex-1].text) + ' ' + str(doc[docIndex].text)\n",
    "\n",
    "\n",
    "                ## NON-WORDS\n",
    "                # Number of non-alphabetized words (numbers)\n",
    "                if token.is_ascii == False:\n",
    "                    numNonASCIIWords = numNonASCIIWords + 1\n",
    "\n",
    "                # digits\n",
    "                if token.is_digit == True:\n",
    "                    numNumericWords = numNumericWords + 1\n",
    "                \n",
    "                # symbols\n",
    "                if token.pos_ == 'SYM':\n",
    "                    numSymbols = numSymbols + 1\n",
    "\n",
    "                # Grab the important word lengths\n",
    "                importantWordLengths.append(len(token))\n",
    "\n",
    "            elif token.is_stop == True:\n",
    "                numStopWords = numStopWords + 1\n",
    "\n",
    "            # Get all the word lengths\n",
    "            allWordLengths.append(len(token))\n",
    "\n",
    "            # increment\n",
    "            docIndex=docIndex+1\n",
    "        # END - for token in doc:\n",
    "        \n",
    "\n",
    "        avgImportantWordLength = 0\n",
    "        avgAllWordLengths = 0\n",
    "        \n",
    "        # Grab aggregates\n",
    "        if numWords > 0:\n",
    "            avgImportantWordLength = np.average(importantWordLengths)\n",
    "            avgAllWordLengths = np.average(allWordLengths)\n",
    "        else:\n",
    "            avgImportantWordLength = 0\n",
    "            avgAllWordLengths = 0\n",
    "            \n",
    "        #print('Words: ' + str(numWords) + '.  Stop: ' + str(numStopWords) \\\n",
    "        #      + '.  Noun: ' + str(numNouns) + '.  Verb: ' + str(numVerbs) + '. Ajd: ' + str(numAdjs) \\\n",
    "        #      + '. UpperCase: ' + str(numUpperCaseWords) + '. Condition: ' + wordsCondition  \\\n",
    "        #      + '. Numeric: ' + str(numNumericWords) + '. Symbols: ' + str(numSymbols) + '. NonASCII: ' + str(numNonASCIIWords) )\n",
    "\n",
    "        #print('Avg word length (important): ' + str(np.average(importantWordLengths)))\n",
    "        #print('Avg word length (all): ' + str(np.average(allWordLengths)))\n",
    "\n",
    "        #df.iat[i, df.columns.get_loc('desc_numWords')] = numWords\n",
    "        df.at[i,'desc_numWords'] = numWords\n",
    "        df.at[i,'desc_numStopWords'] = numStopWords\n",
    "\n",
    "        df.at[i,'desc_numNouns'] = numNouns\n",
    "        df.at[i,'desc_numVerbs'] = numVerbs\n",
    "        df.at[i,'desc_numAdjs'] = numAdjs\n",
    "\n",
    "        df.at[i,'desc_numUpperCaseWords'] = numUpperCaseWords\n",
    "        df.at[i,'desc_wordsCondition'] = wordsCondition\n",
    "\n",
    "        df.at[i,'desc_numNumericWords'] = numNumericWords        \n",
    "        df.at[i,'desc_numSymbols'] = numSymbols\n",
    "        df.at[i,'desc_numNonASCIIWords'] = numNonASCIIWords\n",
    "\n",
    "        df.at[i,'desc_wordBargainOrDeal'] = ''\n",
    "\n",
    "        df.at[i,'desc_avgImportantWordLength'] = avgImportantWordLength\n",
    "        df.at[i,'desc_avgAllWordLength'] = avgAllWordLengths\n",
    "\n",
    "           \n",
    "        # SENTENCES\n",
    "        # Let's look at sentence structure\n",
    "        numSentences = 0\n",
    "\n",
    "        # Number of sentences\n",
    "        numSentences = sum(1 for sentInde in doc.sents)\n",
    "\n",
    "        wordCount = 0\n",
    "        arrWordCount = []\n",
    "\n",
    "        wordLength = 0\n",
    "        arrWordLength = []\n",
    "\n",
    "        sentenceWordCountsArr = []\n",
    "        sentenceAvgWordLengthArr = []\n",
    "\n",
    "        for sent in doc.sents:\n",
    "            wordLength = 0\n",
    "            wordCount = 0\n",
    "            arrWordLength = []\n",
    "\n",
    "            # Words Iteration\n",
    "            for token in sent.subtree:\n",
    "                if token.is_stop == False and token.pos_!='PUNCT' and token.is_ascii == True:\n",
    "                    \n",
    "                    wordLength = 0\n",
    "                    wordCount = wordCount + 1\n",
    "                    arrWordLength.append(len(token))\n",
    "            # End Word Iteration\n",
    "\n",
    "            sentenceCount = sentenceCount + 1\n",
    "            sentenceWordCountsArr.append(wordCount)\n",
    "            sentenceAvgWordLengthArr.append(np.average(arrWordLength))\n",
    "\n",
    "            #if wordCount > 0:\n",
    "                #print('Stat: Total Words for this sentence: ' + str(wordCount))\n",
    "                #print('Stat: Avg of all word lengths for this sentence: ' + str(np.average(arrWordLength)))\n",
    "\n",
    "        avgSentenceWordCounts = 0\n",
    "        avgSentenceAvgWordLength = 0\n",
    "        \n",
    "        if numSentences > 0:\n",
    "            avgSentenceWordCounts = np.average(sentenceWordCountsArr)\n",
    "            avgSentenceAvgWordLength = np.average(sentenceAvgWordLengthArr)\n",
    "        else:\n",
    "            avgSentenceWordCounts = 0\n",
    "            avgSentenceAvgWordLength = 0\n",
    "            \n",
    "        #print('Total Sentences: ' + str(sentenceCount))\n",
    "        #print('Stat Agg: Avg Sentence Word Counts: ' + str(np.average(sentenceWordCountsArr)))\n",
    "        #print('Stat Agg: Avg Sentence Word Length: ' + str(np.average(sentenceAvgWordLengthArr)))  # Average the sentence avg\n",
    "\n",
    "        df.at[i,'desc_numSentences'] = numSentences\n",
    "        df.at[i,'desc_avgWordsPerSentence'] = avgSentenceWordCounts\n",
    "        df.at[i,'desc_avgWordLengthPerSentence'] = avgSentenceAvgWordLength\n",
    "\n",
    "        # Set the processed\n",
    "        df.at[i, 'text_description_processed'] = 1\n",
    "\n",
    "        # only run on the second column, not each column for the checkpoint\n",
    "        if i!=0 and i % 50000 == 0:\n",
    "            print('Processed row (' + str(datetime.now()) + '): ' + str(i) + ' and creating file snapshot. ' + str(datetime.now() - startTime))\n",
    "\n",
    "            file = codecs.open(groupedFileName, 'w', 'utf-8') \n",
    "            df.to_csv(file, index=False)\n",
    "            file.close()\n",
    "     \n",
    "    # END PROCESSING LOOP\n",
    "    \n",
    "        # Test 25\n",
    "        #if i !=0 and i % 20==0:\n",
    "        #    print('exiting')\n",
    "        #    break\n",
    "\n",
    "# END DF ITERATION LOOP\n",
    "print('Finished: ' + str(datetime.now() - startTime) + ' - Now Writing final file ...')\n",
    "# At the end of the processing\n",
    "file = codecs.open(groupedFileName, 'w', 'utf-8') \n",
    "df.to_csv(file, index=False)\n",
    "file.close()\n",
    "print('Finished Writing File')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case it errors out\n",
    "print('Finished: ' + str(datetime.now() - startTime) + ' - Now Writing final file ...')\n",
    "# At the end of the processing\n",
    "file = codecs.open(groupedFileName, 'w', 'utf-8') \n",
    "df.to_csv(file, index=False)\n",
    "file.close()\n",
    "print('Finished Writing File')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
