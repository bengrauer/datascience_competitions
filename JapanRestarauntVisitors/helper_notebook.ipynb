{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONST_TEST_MODE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import calendar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "import platform\n",
    "import sys\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "import numbers\n",
    "\n",
    "from pandas import Series\n",
    "import datetime as dt\n",
    "\n",
    "# Multiple dataframes in single cell\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Auto size the plots to be able to see better\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 6\n",
    "\n",
    "CONST_STATIONARY = 'STATIONARY'\n",
    "CONST_NON_STATIONARY = 'NON_STATIONARY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading, Merging and Massaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return the load of all data - used across mulitiple notebooks as I try different scripts.\n",
    "# This will load, format the data, and then add any additional columns or features\n",
    "# This was originally done in another notebook, and also loaded into SQL for data merge checking\n",
    "def fn_load_all_data(loadDummyVariables=0):\n",
    "\n",
    "    # Working Dir / Load Data - in case we need to distribute later on\n",
    "    if (platform.system() == 'Windows'):\n",
    "        currentOS = 'Windows'\n",
    "        workDir = 'D:\\\\project\\\\data\\\\kg_jpn_rest\\\\'\n",
    "    # MAC\n",
    "    elif (platform.system() == 'Darwin'):\n",
    "        currentOS = 'Mac'\n",
    "        workDir = '//Project/data/kg_jpn_rest/'\n",
    "    # AWS\n",
    "    elif (platform.system() == 'Linux'):\n",
    "        currentOS = 'Linux'\n",
    "        workDir = '//data/'\n",
    "\n",
    "    print('Data Load - Loading data on: ' + currentOS + ' Directory: ' + workDir)\n",
    "\n",
    "    # reservations in the air system\n",
    "    pd_air_reserve = pd.read_csv(workDir + 'air_reserve.csv', parse_dates=(['visit_datetime','reserve_datetime']), infer_datetime_format=True)\n",
    "    pd_air_reserve.name = 'pd_air_reserve'\n",
    "    # reservation in the hpg system\n",
    "    pd_hpg_reserve = pd.read_csv(workDir + 'hpg_reserve.csv', parse_dates=(['visit_datetime','reserve_datetime']), infer_datetime_format=True)\n",
    "    pd_hpg_reserve.name = 'pd_hpg_reserve'\n",
    "    # contains info about the store info.  lat and long is the area which the store belongs\n",
    "    pd_air_store_info = pd.read_csv(workDir + 'air_store_info.csv', infer_datetime_format=True)\n",
    "    pd_air_store_info.name = 'pd_air_store_info'\n",
    "    # contains info about select air restaraunts.  Lat and long is the area where store belongs\n",
    "    pd_hpg_store_info = pd.read_csv(workDir + 'hpg_store_info.csv', infer_datetime_format=True)\n",
    "    pd_hpg_store_info.name = 'pd_hpg_store_info'\n",
    "    # file contains HISTORICAL visit data for the air restaraunts\n",
    "    pd_air_visit_data = pd.read_csv(workDir + 'air_visit_data.csv', parse_dates=(['visit_date']), infer_datetime_format=True)\n",
    "    pd_air_visit_data.name = 'pd_air_visit_data'\n",
    "    # give basic info about the calendar dates in the dataset\n",
    "    pd_date_info = pd.read_csv(workDir + 'date_info.csv', parse_dates=(['calendar_date']), infer_datetime_format=True)\n",
    "    pd_date_info.name = 'pd_date_info'\n",
    "    # allows you to join select restaraunts that have both air and hpg systems\n",
    "    pd_store_id_relation = pd.read_csv(workDir + 'store_id_relation.csv', infer_datetime_format=True)\n",
    "    pd_store_id_relation.name = 'pd_store_id_relation'\n",
    "\n",
    "    # Flatten the hpg reserve and air reserve\n",
    "    # add a date\n",
    "    pd_hpg_reserve.sort_values(['hpg_store_id','visit_datetime'], ascending=[True,True], inplace=True)\n",
    "    pd_hpg_reserve['visit_date'] = pd_hpg_reserve['visit_datetime'].dt.date\n",
    "    hpgPivot = pd.pivot_table(pd_hpg_reserve, \n",
    "                              values='reserve_visitors', \n",
    "                              index=['hpg_store_id','visit_date'],\n",
    "                              aggfunc=np.sum)\n",
    "    hpgPivot.reset_index(inplace=True)\n",
    "    hpgPivot['visit_date'] = hpgPivot['visit_date'].astype('datetime64[ns]')\n",
    "    hpgPivot.head()\n",
    "\n",
    "\n",
    "    pd_air_reserve.sort_values(['air_store_id','visit_datetime'], ascending=[True,True], inplace=True)\n",
    "    pd_air_reserve['visit_date'] = pd_air_reserve['visit_datetime'].dt.date\n",
    "    airPivot = pd.pivot_table(pd_air_reserve, \n",
    "                              values='reserve_visitors', \n",
    "                              index=['air_store_id','visit_date'],\n",
    "                              aggfunc=np.sum)\n",
    "    airPivot.reset_index(inplace=True)\n",
    "    airPivot['visit_date'] = airPivot['visit_date'].astype('datetime64[ns]')\n",
    "    airPivot.head()\n",
    "\n",
    "    # First set the superjoin to the air_visit_data\n",
    "    dfSuper = pd_air_visit_data\n",
    "\n",
    "    \n",
    "    print('Data Load - Merging Data')\n",
    "    # join visit + store xmap\n",
    "    dfSuper = pd.merge(dfSuper, pd_store_id_relation, how='left', on ='air_store_id')\n",
    "    # join hpg store info\n",
    "    dfSuper = pd.merge(dfSuper, pd_hpg_store_info, how='left', on='hpg_store_id')\n",
    "    # join air store info\n",
    "    dfSuper = pd.merge(dfSuper, pd_air_store_info, how='left', on='air_store_id')\n",
    "    # now the reservations\n",
    "    dfSuper = pd.merge(dfSuper, hpgPivot, how='left', on=['hpg_store_id','visit_date'])\n",
    "    dfSuper = pd.merge(dfSuper, airPivot, how='left', on=['air_store_id','visit_date'])\n",
    "    # now the holiday\n",
    "    dfSuper = pd.merge(dfSuper, pd_date_info, how='left', left_on='visit_date', right_on='calendar_date')\n",
    "\n",
    "    # rename the columns\n",
    "    dfSuper.rename(columns={'latitude_x':'hpg_latitude', 'longitude_x':'hpg_longitude', \n",
    "                           'latitude_y':'air_latitude', 'longitude_y':'air_longitude',\n",
    "                           'reserve_visitors_x':'hpg_reserve_visitors', 'reserve_visitors_y':'air_reserve_visitors' }, inplace=True)\n",
    "\n",
    "    # Set any numbers that are na\n",
    "    dfSuper['visitors'].replace({np.nan: 0}, inplace=True)\n",
    "    dfSuper['air_latitude'].replace({np.nan: 0}, inplace=True)\n",
    "    dfSuper['air_longitude'].replace({np.nan: 0}, inplace=True)\n",
    "    dfSuper['hpg_latitude'].replace({np.nan: 0}, inplace=True)\n",
    "    dfSuper['hpg_longitude'].replace({np.nan: 0}, inplace=True)\n",
    "    dfSuper['air_reserve_visitors'].replace({np.nan: 0}, inplace=True)\n",
    "    dfSuper['hpg_reserve_visitors'].replace({np.nan: 0}, inplace=True)\n",
    "\n",
    "    # blank strings\n",
    "    dfSuper['air_genre_name'].replace({np.nan: ''}, inplace=True)\n",
    "    dfSuper['air_area_name'].replace({np.nan: ''}, inplace=True)\n",
    "    dfSuper['hpg_genre_name'].replace({np.nan: ''}, inplace=True)\n",
    "    dfSuper['hpg_area_name'].replace({np.nan: ''}, inplace=True)\n",
    "\n",
    "    \n",
    "\n",
    "    # now if we have restaranut info data between the two reservation (AIR + HPG) systems for the same restaraunt, \n",
    "    #   take the AIR as the tie-breaker\n",
    "    # will take in two columns.  If 1 has data present, use that, else if col 2 has data, return that\n",
    "    def fn_assign_air_hpg_value(colAir, colHpg):\n",
    "\n",
    "        # if a number\n",
    "        if isinstance(colAir, numbers.Number) and isinstance(colHpg, numbers.Number):\n",
    "            if colAir > 0:\n",
    "                return colAir\n",
    "            elif colHpg > 0:\n",
    "                return colHpg\n",
    "            else:\n",
    "                return 0;\n",
    "\n",
    "        # if it's a string\n",
    "        else:\n",
    "            if len(colAir) > 0:\n",
    "                return colAir\n",
    "            elif len(colHpg) > 0:\n",
    "                return colHpg\n",
    "            else:\n",
    "                return ''\n",
    "\n",
    "    # Define if it is a weekend (Friday, Sat, Sun)\n",
    "    def fn_is_weekend(col):\n",
    "        if ((col==4) or (col==5) or (col==6)):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    # Start to combine\n",
    "    dfSuper['genre_name\t'] = dfSuper.apply(lambda row: fn_assign_air_hpg_value(row['air_genre_name'], row['hpg_genre_name']), axis=1)\n",
    "    dfSuper['area_name'] = dfSuper.apply(lambda row: fn_assign_air_hpg_value(row['air_area_name'], row['hpg_area_name']), axis=1)\n",
    "    dfSuper['latitude'] = dfSuper.apply(lambda row: fn_assign_air_hpg_value(row['air_latitude'], row['hpg_latitude']), axis=1)\n",
    "    dfSuper['longitude'] = dfSuper.apply(lambda row: fn_assign_air_hpg_value(row['air_longitude'], row['hpg_longitude']), axis=1)\n",
    "\n",
    "    # Add the reservations between the two systems\n",
    "    dfSuper['reserve_visitors'] = dfSuper['hpg_reserve_visitors'] + dfSuper['air_reserve_visitors']\n",
    "\n",
    "    print('Data Load - Adding Features')\n",
    "    # Features\n",
    "    # Add day and month\n",
    "    dfSuper['month_num'] = dfSuper['visit_date'].dt.month\n",
    "    dfSuper['month_name'] = dfSuper['month_num'].apply(lambda x: calendar.month_abbr[x])\n",
    "    \n",
    "    dfSuper['dayofmonth_num'] = dfSuper['visit_date'].dt.day\n",
    "    dfSuper['dayofweek_num'] = dfSuper['visit_date'].dt.dayofweek  # Monday is 0, Sunday is 6\n",
    "\n",
    "    # Weekend\n",
    "    dfSuper['weekend'] = dfSuper.apply(lambda row: fn_is_weekend(row['dayofweek_num']), axis=1)\n",
    "\n",
    "    # Correlations\n",
    "    # reservations to visitors correlation\n",
    "    dfCorr = dfSuper.copy()\n",
    "    # shorten the list\n",
    "    dfCorr = dfCorr[['air_store_id','visitors','reserve_visitors']]\n",
    "    # re-create dataframe from correlation grouped by the air_store_id\n",
    "    dfCorr = dfCorr.groupby('air_store_id')[['visitors','reserve_visitors']].corr().iloc[0::2]\n",
    "    # rename the colum\n",
    "    dfCorr = dfCorr.rename(columns={'reserve_visitors':'corr_vis_resv'})\n",
    "    # drop the \"visitors\" columns that are left over from the matrix\n",
    "    dfCorr.reset_index(inplace=True)\n",
    "    dfCorr.drop(['visitors','level_1'], axis=1, inplace=True)\n",
    "    # set any nan to 0\n",
    "    dfCorr['corr_vis_resv'].replace({np.nan: 0}, inplace=True)\n",
    "    # join back\n",
    "    dfSuper= pd.merge(dfSuper, dfCorr, how='left', on ='air_store_id', )\n",
    "    dfSuper.rename(columns={'corr_vis_resv_x':'corr_vis_resv'}, inplace=True)\n",
    "\n",
    "    \n",
    "    # Drop the duplicated columns\n",
    "    dfSuper.drop(['hpg_store_id','hpg_genre_name','hpg_area_name','hpg_latitude','hpg_longitude','air_genre_name',\n",
    "                   'air_area_name','air_latitude','air_longitude', 'hpg_reserve_visitors','air_reserve_visitors'], axis=1, inplace=True)\n",
    "\n",
    "    # Set the indexes for the data\n",
    "    # Set Indexes\n",
    "    dfSuper.set_index('visit_date', inplace=True)\n",
    "\n",
    "    # Set any floats for the data (to work with time-series)\n",
    "    # Set data types\n",
    "    dfSuper['visitors'] = dfSuper['visitors'].astype('float32')\n",
    "\n",
    "    \n",
    "    # do each one in a consolidated\n",
    "    def convert_dummy_vars(dataFrame, columnName, dropFirst=False, colRename=''):\n",
    "\n",
    "        temp = pd.get_dummies(dataFrame[columnName], drop_first=dropFirst)\n",
    "\n",
    "        if colRename != '':\n",
    "            temp.columns = [(colRename)]\n",
    "\n",
    "        #dataFrame.drop([columnName], axis=1, inplace=True)\n",
    "        dataFrame = pd.concat([dataFrame, temp], axis=1)    \n",
    "        return dataFrame\n",
    "    \n",
    "    # Hit the dummy variables\n",
    "    if loadDummyVariables==1:\n",
    "        dfSuper = convert_dummy_vars(dataFrame=dfSuper, columnName='day_of_week')\n",
    "        dfSuper = convert_dummy_vars(dataFrame=dfSuper, columnName='month_name')\n",
    "        \n",
    "    \n",
    "    # Create columns and default (initialize)\n",
    "    dfSuper['forecast'] = 0\n",
    "    dfSuper['forecast'] = dfSuper['forecast'].astype('float32')    \n",
    "        \n",
    "    print('Data Load - Finished')\n",
    "    \n",
    "    # Final Re-Organization of columns\n",
    "    \n",
    "    return dfSuper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function above\n",
    "if CONST_TEST_MODE==1:\n",
    "    df = fn_load_all_data(1)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorgder the columns\n",
    "#df = df[['air_store_id','visitors','reserve_visitors','corr_vis_resv','genre_name','area_name','latitude','longitude',\\\n",
    "#    'calendar_date','holiday_flg','month_num','month_name','dayofmonth_num','day_of_week','dayofweek_num','weekend',\\\n",
    "#    'forecast']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do each one in a consolidated\n",
    "def convert_dummy_vars(dataFrame, columnName, dropFirst=False, colRename=''):\n",
    "    \n",
    "    temp = pd.get_dummies(dataFrame[columnName], drop_first=dropFirst)\n",
    "    \n",
    "    if colRename != '':\n",
    "        temp.columns = [(colRename)]\n",
    "    \n",
    "    #dataFrame.drop([columnName], axis=1, inplace=True)\n",
    "    dataFrame = pd.concat([dataFrame, temp], axis=1)    \n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nben = df.copy()\\nben.head()\\n\\n# CHOOSE WHICH FEATURES TO INCLUDE\\n# 'amount_tsh', - leave out - add after binning and plotting\\n\\nshortList_v1 = ['day_of_week','month_name']\\nben = convert_dummy_vars(dataFrame=ben, columnName='day_of_week')\\nben = convert_dummy_vars(dataFrame=ben, columnName='month_name')\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ben = df.copy()\n",
    "ben.head()\n",
    "\n",
    "# CHOOSE WHICH FEATURES TO INCLUDE\n",
    "# 'amount_tsh', - leave out - add after binning and plotting\n",
    "\n",
    "shortList_v1 = ['day_of_week','month_name']\n",
    "ben = convert_dummy_vars(dataFrame=ben, columnName='day_of_week')\n",
    "ben = convert_dummy_vars(dataFrame=ben, columnName='month_name')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA / Plot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION\n",
    "def fn_ts_add_cycletrend_analysis(df, colName):\n",
    "        \n",
    "    # cycle and trend\n",
    "    cycle, trend = sm.tsa.filters.hpfilter(df[colName])\n",
    "    df['cycle'] = cycle\n",
    "    df['trend'] = trend\n",
    "    \n",
    "    # Simple moving average\n",
    "    df['3-day-SMA'] = df[colName].rolling(window=3).mean()\n",
    "    df['7-day-SMA'] = df[colName].rolling(window=7).mean()\n",
    "    df['14-day-SMA'] = df[colName].rolling(window=14).mean()\n",
    "    df['31-day-SMA'] = df[colName].rolling(window=31).mean()\n",
    "    \n",
    "    # EWMA\n",
    "    df['EWMA_7_days'] = df[colName].ewm(span=7).mean()\n",
    "    df['EWMA_14_days'] = df[colName].ewm(span=14).mean()\n",
    "    df['EWMA_31_days'] = df[colName].ewm(span=31).mean()\n",
    "    \n",
    "    newColList = ['cycle','trend','3-day-SMA','7-day-SMA','14-day-SMA','31-day-SMA',\n",
    "                 'EWMA_7_days','EWMA_14_days','EWMA_31_days']\n",
    "    \n",
    "    return df, newColList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "#df = fn_ts_add_cycletrend_analysis(df, 'visitors')\n",
    "#df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ad-fuller Check\n",
    "def fn_adf_check(time_series):\n",
    "    \"\"\"\n",
    "    Pass in a time series, returns ADF report\n",
    "    \"\"\"\n",
    "    result = adfuller(time_series)\n",
    "    print('Augmented Dickey-Fuller Test:')\n",
    "    labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n",
    "\n",
    "    for value,label in zip(result,labels):\n",
    "        print(label+' : '+str(value) )\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\")\n",
    "        return CONST_STATIONARY\n",
    "    else:\n",
    "        print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")\n",
    "        return CONST_NON_STATIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_plot_auto_corr(df):\n",
    "    autocorrelation_plot(df.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_plot_acf(df):\n",
    "    plot = plot_acf(df.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_plot_pacf(df):\n",
    "    plot = plot_pacf(df.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_plot_decomposition(series, frequency):    \n",
    "    decomposition1 = seasonal_decompose(series, freq=frequency)\n",
    "    fig = plt.figure()\n",
    "    fig = decomposition1.plot()\n",
    "    fig.set_size_inches(20,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_plot_result_diagnostics(model_results):\n",
    "    fPlot = model_results.plot_diagnostics(figsize=(10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONST_TEST_MODE==1:\n",
    "    subDF = df[df['air_store_id']=='air_1c0b150f9e696a5f']\n",
    "    \n",
    "    fn_plot_auto_corr(subDF['visitors'])\n",
    "    fn_plot_acf(subDF['visitors'])\n",
    "    fn_plot_pacf(subDF['visitors'])\n",
    "    fn_plot_decomposition(subDF['visitors'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models to Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not yet tested / implemented\n",
    "def fn_run_arima_timeseries(visitMergeDF, storeToEvaluate, orderList, logger):\n",
    "    \n",
    "    errorOccured = False\n",
    "    \n",
    "    if GLOBAL_CONST_DEBUG ==1:\n",
    "        print('Start Processing Restaraunt: ' + str(storeToEvaluate))\n",
    "    \n",
    "    # create ts sub-set\n",
    "    ts = visitMergeDF[visitMergeDF['air_store_id']==storeToEvaluate].copy()\n",
    "    ts.asfreq('D')\n",
    "    \n",
    "    minDate = ts.index.min()\n",
    "    \n",
    "    # add dates to predict\n",
    "    idx = pd.DataFrame(pd.date_range('2017-04-23','2017-05-31'), columns={'dateRange'})\n",
    "    idx.set_index('dateRange',inplace=True)\n",
    "    ts = pd.concat([ts,idx], axis=1)\n",
    "\n",
    "    # Impute\n",
    "    ts['air_store_id'].replace({np.nan: storeToEvaluate}, inplace=True)\n",
    "    ts['visitors'].replace({np.nan: 0}, inplace=True)\n",
    "    ts['visitors_log'].replace({np.nan: 0}, inplace=True)\n",
    "    \n",
    "    # Try Catch Here\n",
    "    try:\n",
    "    \n",
    "        # standard ARIMA model with order list passed in\n",
    "        model = ARIMA(ts['visitors'][minDate:\"2017-04-23\"], order=orderList)\n",
    "        modelFit = model.fit(disp=-1)\n",
    "        # 39 days is how far out we are predicting\n",
    "        results = modelFit.forecast(39)  \n",
    "        # set forecast\n",
    "        ts['forecast'][\"2017-04-23\":\"2017-05-31\"] = results[0][:]\n",
    "\n",
    "        if GLOBAL_CONST_DEBUG ==1:\n",
    "            print('Start Processing Restaraunt - LOG: ' + str(storeToEvaluate))\n",
    "\n",
    "        # Do the log while we are in here\n",
    "        modelLog = ARIMA(ts['visitors_log'][minDate:\"2017-04-23\"], order=orderList)\n",
    "        modelLog_fit = modelLog.fit(disp=-1)\n",
    "        resultsLog = modelLog_fit.forecast(39)\n",
    "        ts['forecast_log'][\"2017-04-23\":\"2017-05-31\"] = resultsLog[0][:]\n",
    "        ts['forecast_logExp'] = np.exp(ts['forecast_log'])  # Revert log back to standard\n",
    "    \n",
    "    \n",
    "    except: \n",
    "        #(RuntimeError, TypeError, NameError):\n",
    "        #print('Error')\n",
    "        #print(RuntimeError)\n",
    "        #print(TypeError)\n",
    "        #print(NameError)\n",
    "        #print('\\n')\n",
    "        logger.info('Error processing store: ' + storeToEvaluate)\n",
    "        errorOccured = True\n",
    "           \n",
    "    pass\n",
    "\n",
    "\n",
    "    if GLOBAL_CONST_DEBUG ==1:\n",
    "        print('Finished Processing Restaraunt: ' + str(storeToEvaluate))\n",
    "\n",
    "\n",
    "    # return back only what we predicted\n",
    "    dfTSReturn = ts[:][\"2017-04-23\":]\n",
    "    return dfTSReturn, errorOccured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalization of Submission Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will finalize the submission file\n",
    "def fn_finalize_submission_file(exportDir, colNameToSubmit):\n",
    "    \n",
    "    arimaExportDir = 'D:\\\\project\\\\data\\\\kg_jpn_rest\\\\export\\\\'\n",
    "    arimaExportFile = 'export_results.csv'\n",
    "\n",
    "    armRsltDF = pd.read_csv(arimaExportDir + arimaExportFile)\n",
    "    pd_sample_submission = pd.read_csv('D:\\\\project\\\\data\\\\kg_jpn_rest\\\\' + 'sample_submission.csv')\n",
    "    \n",
    "    # Concatenate the fields to create the ID\n",
    "    armRsltDF['id'] = ''\n",
    "    armRsltDF['id'] = armRsltDF['id'].str.cat(armRsltDF['air_store_id'])\n",
    "    armRsltDF['id'] = armRsltDF['id'] + '_'\n",
    "    armRsltDF['id'] = armRsltDF['id'].str.cat(armRsltDF['visit_date'].astype(str))\n",
    "\n",
    "    # Drop the visitors (will rename later)\n",
    "    armRsltDF.drop('visitors', axis=1, inplace=True)\n",
    "\n",
    "    # Must turn any negatives into a zero\n",
    "    # Not sure why this needs to come after, perhaps with the merge it throws some things between float/int offf\n",
    "    def fn_set_negative_to_zero(col):\n",
    "        if col < 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return col\n",
    "\n",
    "    # Took out the log items for now    \n",
    "    armRsltDF['forecast'] = armRsltDF.apply(lambda row: fn_set_negative_to_zero(row['forecast']), axis=1)\n",
    "    armRsltDF[armRsltDF['forecast']<0]['forecast'] = 0.0\n",
    "\n",
    "    # Join to sample submission\n",
    "    armRsltDF = pd.merge(pd_sample_submission, armRsltDF, how='inner', on=('id'))\n",
    "\n",
    "    # Drop the visitors (will rename later)\n",
    "    armRsltDF.drop('visitors', axis=1, inplace=True)\n",
    "\n",
    "    # 32019 - correct length\n",
    "    if (len(armRsltDF)) == 32019:\n",
    "        print('Correct Submission Length')\n",
    "    else:\n",
    "        print('!! ERROR !! - Incorrect Submission Length')\n",
    "\n",
    "    def fn_write_submission_file(df, colNames, fileName):\n",
    "\n",
    "        print(colNames[1])\n",
    "        forecastCol = colNames[1]\n",
    "\n",
    "        tempDF = df.copy()\n",
    "\n",
    "        # Rename the second column to \"visitors\" as per submission\n",
    "        tempDF.rename(columns={forecastCol: 'visitors'}, inplace=True)\n",
    "\n",
    "        #print(tempDF.head())\n",
    "        tempDF.to_csv(fileName, header=True, index=False, quotechar='\"', columns=('id','visitors'))\n",
    "        print('Wrote file: ' + fileName)\n",
    "\n",
    "    todayDate = str(dt.date.today().strftime('%Y%m%d'))\n",
    "\n",
    "    # Float forecast\n",
    "    exportDF = armRsltDF.copy()\n",
    "    exportDF\n",
    "    columns=('id','forecast')\n",
    "    fn_write_submission_file(armRsltDF, columns, arimaExportDir + todayDate + '_subm_frcst_flt.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
