{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONST_TEST_MODE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import calendar\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "import platform\n",
    "import sys\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "import numbers\n",
    "\n",
    "from pandas import Series\n",
    "import datetime as dt\n",
    "import warnings\n",
    "\n",
    "# Multiple dataframes in single cell\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Auto size the plots to be able to see better\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 6\n",
    "\n",
    "CONST_STATIONARY = 'STATIONARY'\n",
    "CONST_NON_STATIONARY = 'NON_STATIONARY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display time\n",
    "def print_elapsed_time(elapsed_time):\n",
    "    if elapsed_time > 60:\n",
    "        print('Time to Finish: '+str(elapsed_time/60) + ' min and ' + str(elapsed_time/(60*2)) + ' seconds' )\n",
    "    else:\n",
    "        print('Time to Finish: '+str(elapsed_time) + ' seconds') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading, Merging and Massaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION\n",
    "# 01/14/2018 - Ben Grauer - changed the sma constant type\n",
    "def fn_ts_add_cycletrend_analysis_subfunction(df, evalColName, newColName, window, isSMA, isEWMA):\n",
    "    if isSMA==1:\n",
    "        df[newColName] = df[evalColName].rolling(window=window).mean()\n",
    "    elif isEWMA==1:\n",
    "        df[newColName] = df[evalColName].ewm(span=window).mean()\n",
    "    df[newColName].replace({np.nan: 0.0}, inplace=True)\n",
    "    #df[colName] = df[colName].astype('float32')\n",
    "    return df\n",
    "    \n",
    "def fn_ts_add_cycletrend_analysis(df, colName):\n",
    "\n",
    "    #df = inputDF.copy()\n",
    "    \n",
    "    # cycle and trend\n",
    "    \n",
    "    cycle, trend = sm.tsa.filters.hpfilter(df[colName])\n",
    "    df['cycle'] = cycle\n",
    "    df['trend'] = trend\n",
    "\n",
    "    # Simple moving average\n",
    "    df = fn_ts_add_cycletrend_analysis_subfunction(df, 'visitors', 'SMA_3_days', 3, isSMA=1, isEWMA=0)\n",
    "    df = fn_ts_add_cycletrend_analysis_subfunction(df, 'visitors', 'SMA_7_days', 7, isSMA=1, isEWMA=0)\n",
    "    df = fn_ts_add_cycletrend_analysis_subfunction(df, 'visitors', 'SMA_14_days', 14, isSMA=1, isEWMA=0)\n",
    "    df = fn_ts_add_cycletrend_analysis_subfunction(df, 'visitors', 'SMA_30_days', 30, isSMA=1, isEWMA=0)\n",
    "    df = fn_ts_add_cycletrend_analysis_subfunction(df, 'visitors', 'SMA_90_days', 90, isSMA=1, isEWMA=0)\n",
    "    \n",
    "    df = fn_ts_add_cycletrend_analysis_subfunction(df, 'visitors', 'EWMA_3_days', 3, isSMA=0, isEWMA=1)\n",
    "    df = fn_ts_add_cycletrend_analysis_subfunction(df, 'visitors', 'EWMA_7_days', 7, isSMA=0, isEWMA=1)\n",
    "    df = fn_ts_add_cycletrend_analysis_subfunction(df, 'visitors', 'EWMA_14_days', 14, isSMA=0, isEWMA=1)\n",
    "    df = fn_ts_add_cycletrend_analysis_subfunction(df, 'visitors', 'EWMA_30_days', 30, isSMA=0, isEWMA=1)\n",
    "    df = fn_ts_add_cycletrend_analysis_subfunction(df, 'visitors', 'EWMA_90_days', 90, isSMA=0, isEWMA=1)\n",
    "\n",
    "    return df #, newColList\n",
    "\n",
    "# Monday starts at 0, and then Sunday is 6\n",
    "# Define if it is a weekend (Friday, Sat, Sun)\n",
    "def fn_is_weekend(col):\n",
    "    if ((col==4) or (col==5) or (col==6)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# do each one in a consolidated\n",
    "def convert_dummy_vars(dataFrame, columnName, dropFirst=False, colRename=''):\n",
    "\n",
    "    temp = pd.get_dummies(dataFrame[columnName], drop_first=dropFirst)\n",
    "\n",
    "    if colRename != '':\n",
    "        temp.columns = [(colRename)]\n",
    "\n",
    "    #dataFrame.drop([columnName], axis=1, inplace=True)\n",
    "    dataFrame = pd.concat([dataFrame, temp], axis=1)    \n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working Dir / Load Data - in case we need to distribute later on\n",
    "if (platform.system() == 'Windows'):\n",
    "    currentOS = 'Windows'\n",
    "    workDir = 'D:\\\\project\\\\data\\\\kg_jpn_rest\\\\'\n",
    "# MAC\n",
    "elif (platform.system() == 'Darwin'):\n",
    "    currentOS = 'Mac'\n",
    "    workDir = '//Project/data/kg_jpn_rest/'\n",
    "# AWS\n",
    "elif (platform.system() == 'Linux'):\n",
    "    currentOS = 'Linux'\n",
    "    workDir = '//data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return the load of all data - used across mulitiple notebooks as I try different scripts.\n",
    "# This will load, format the data, and then add any additional columns or features\n",
    "# This was originally done in another notebook, and also loaded into SQL for data merge checking\n",
    "def fn_load_all_data(loadDummyVariables=1):\n",
    "\n",
    "    startTime = datetime.now()\n",
    "    \n",
    "    ####################################\n",
    "    print('Data Load - Loading data on: ' + currentOS + ' Directory: ' + workDir)\n",
    "    ####################################\n",
    "    \n",
    "    # reservations in the air system\n",
    "    pd_air_reserve = pd.read_csv(workDir + 'air_reserve.csv', parse_dates=(['visit_datetime','reserve_datetime']), infer_datetime_format=True)\n",
    "    pd_air_reserve.name = 'pd_air_reserve'\n",
    "    # reservation in the hpg system\n",
    "    pd_hpg_reserve = pd.read_csv(workDir + 'hpg_reserve.csv', parse_dates=(['visit_datetime','reserve_datetime']), infer_datetime_format=True)\n",
    "    pd_hpg_reserve.name = 'pd_hpg_reserve'\n",
    "    # contains info about the store info.  lat and long is the area which the store belongs\n",
    "    pd_air_store_info = pd.read_csv(workDir + 'air_store_info.csv', infer_datetime_format=True)\n",
    "    pd_air_store_info.name = 'pd_air_store_info'\n",
    "    # contains info about select air restaraunts.  Lat and long is the area where store belongs\n",
    "    pd_hpg_store_info = pd.read_csv(workDir + 'hpg_store_info.csv', infer_datetime_format=True)\n",
    "    pd_hpg_store_info.name = 'pd_hpg_store_info'\n",
    "    # file contains HISTORICAL visit data for the air restaraunts\n",
    "    pd_air_visit_data = pd.read_csv(workDir + 'air_visit_data.csv', parse_dates=(['visit_date']), infer_datetime_format=True)\n",
    "    pd_air_visit_data.name = 'pd_air_visit_data'\n",
    "    # give basic info about the calendar dates in the dataset\n",
    "    pd_date_info = pd.read_csv(workDir + 'date_info.csv', parse_dates=(['calendar_date']), infer_datetime_format=True)\n",
    "    pd_date_info.name = 'pd_date_info'\n",
    "    # allows you to join select restaraunts that have both air and hpg systems\n",
    "    pd_store_id_relation = pd.read_csv(workDir + 'store_id_relation.csv', infer_datetime_format=True)\n",
    "    pd_store_id_relation.name = 'pd_store_id_relation'\n",
    "\n",
    "    # Flatten the hpg reserve and air reserve\n",
    "    # add a date\n",
    "    pd_hpg_reserve.sort_values(['hpg_store_id','visit_datetime'], ascending=[True,True], inplace=True)\n",
    "    pd_hpg_reserve['visit_date'] = pd_hpg_reserve['visit_datetime'].dt.date\n",
    "    hpgPivot = pd.pivot_table(pd_hpg_reserve, \n",
    "                              values='reserve_visitors', \n",
    "                              index=['hpg_store_id','visit_date'],\n",
    "                              aggfunc=np.sum)\n",
    "    hpgPivot.reset_index(inplace=True)\n",
    "    hpgPivot['visit_date'] = hpgPivot['visit_date'].astype('datetime64[ns]')\n",
    "    hpgPivot.head()\n",
    "\n",
    "\n",
    "    pd_air_reserve.sort_values(['air_store_id','visit_datetime'], ascending=[True,True], inplace=True)\n",
    "    pd_air_reserve['visit_date'] = pd_air_reserve['visit_datetime'].dt.date\n",
    "    airPivot = pd.pivot_table(pd_air_reserve, \n",
    "                              values='reserve_visitors', \n",
    "                              index=['air_store_id','visit_date'],\n",
    "                              aggfunc=np.sum)\n",
    "    airPivot.reset_index(inplace=True)\n",
    "    airPivot['visit_date'] = airPivot['visit_date'].astype('datetime64[ns]')\n",
    "    airPivot.head()\n",
    "\n",
    "    # First set the superjoin to the air_visit_data\n",
    "    dfSuper = pd_air_visit_data\n",
    "\n",
    "    print (datetime.now() - startTime) \n",
    "    \n",
    "    ####################################\n",
    "    print('Data Load - Merging Data')\n",
    "    ####################################\n",
    "    # join visit + store xmap\n",
    "    dfSuper = pd.merge(dfSuper, pd_store_id_relation, how='left', on ='air_store_id')\n",
    "    # join hpg store info\n",
    "    dfSuper = pd.merge(dfSuper, pd_hpg_store_info, how='left', on='hpg_store_id')\n",
    "    # join air store info\n",
    "    dfSuper = pd.merge(dfSuper, pd_air_store_info, how='left', on='air_store_id')\n",
    "    # now the reservations\n",
    "    dfSuper = pd.merge(dfSuper, hpgPivot, how='left', on=['hpg_store_id','visit_date'])\n",
    "    dfSuper = pd.merge(dfSuper, airPivot, how='left', on=['air_store_id','visit_date'])\n",
    "    # now the holiday\n",
    "    dfSuper = pd.merge(dfSuper, pd_date_info, how='left', left_on='visit_date', right_on='calendar_date')\n",
    "\n",
    "    # rename the columns\n",
    "    dfSuper.rename(columns={'latitude_x':'hpg_latitude', 'longitude_x':'hpg_longitude', \n",
    "                           'latitude_y':'air_latitude', 'longitude_y':'air_longitude',\n",
    "                           'reserve_visitors_x':'hpg_reserve_visitors', 'reserve_visitors_y':'air_reserve_visitors' }, inplace=True)\n",
    "\n",
    "    # Set any numbers that are na\n",
    "    dfSuper['visitors'].replace({np.nan: 0}, inplace=True)\n",
    "    dfSuper['air_latitude'].replace({np.nan: 0}, inplace=True)\n",
    "    dfSuper['air_longitude'].replace({np.nan: 0}, inplace=True)\n",
    "    dfSuper['hpg_latitude'].replace({np.nan: 0}, inplace=True)\n",
    "    dfSuper['hpg_longitude'].replace({np.nan: 0}, inplace=True)\n",
    "    dfSuper['air_reserve_visitors'].replace({np.nan: 0}, inplace=True)\n",
    "    dfSuper['hpg_reserve_visitors'].replace({np.nan: 0}, inplace=True)\n",
    "\n",
    "    # blank strings\n",
    "    dfSuper['air_genre_name'].replace({np.nan: ''}, inplace=True)\n",
    "    dfSuper['air_area_name'].replace({np.nan: ''}, inplace=True)\n",
    "    dfSuper['hpg_genre_name'].replace({np.nan: ''}, inplace=True)\n",
    "    dfSuper['hpg_area_name'].replace({np.nan: ''}, inplace=True)\n",
    "\n",
    "    \n",
    "\n",
    "    # now if we have restaranut info data between the two reservation (AIR + HPG) systems for the same restaraunt, \n",
    "    #   take the AIR as the tie-breaker\n",
    "    # will take in two columns.  If 1 has data present, use that, else if col 2 has data, return that\n",
    "    def fn_assign_air_hpg_value(colAir, colHpg):\n",
    "\n",
    "        # if a number\n",
    "        if isinstance(colAir, numbers.Number) and isinstance(colHpg, numbers.Number):\n",
    "            if colAir > 0:\n",
    "                return colAir\n",
    "            elif colHpg > 0:\n",
    "                return colHpg\n",
    "            else:\n",
    "                return 0;\n",
    "\n",
    "        # if it's a string\n",
    "        else:\n",
    "            if len(colAir) > 0:\n",
    "                return colAir\n",
    "            elif len(colHpg) > 0:\n",
    "                return colHpg\n",
    "            else:\n",
    "                return ''\n",
    "\n",
    "\n",
    "    # Start to combine\n",
    "    dfSuper['genre_name'] = dfSuper.apply(lambda row: fn_assign_air_hpg_value(row['air_genre_name'], row['hpg_genre_name']), axis=1)\n",
    "    dfSuper['area_name'] = dfSuper.apply(lambda row: fn_assign_air_hpg_value(row['air_area_name'], row['hpg_area_name']), axis=1)\n",
    "    dfSuper['latitude'] = dfSuper.apply(lambda row: fn_assign_air_hpg_value(row['air_latitude'], row['hpg_latitude']), axis=1)\n",
    "    dfSuper['longitude'] = dfSuper.apply(lambda row: fn_assign_air_hpg_value(row['air_longitude'], row['hpg_longitude']), axis=1)\n",
    "\n",
    "    # Add the reservations between the two systems\n",
    "    dfSuper['reserve_visitors'] = dfSuper['hpg_reserve_visitors'] + dfSuper['air_reserve_visitors']\n",
    "\n",
    "    print (datetime.now() - startTime) \n",
    "    \n",
    "    ####################################\n",
    "    print('Data Load - Adding Features')\n",
    "    ####################################\n",
    "    # Features\n",
    "    # Add day and month\n",
    "    dfSuper['month_num'] = dfSuper['visit_date'].dt.month\n",
    "    dfSuper['month_name'] = dfSuper['month_num'].apply(lambda x: calendar.month_abbr[x])\n",
    "    \n",
    "    dfSuper['dayofmonth_num'] = dfSuper['visit_date'].dt.day\n",
    "    dfSuper['dayofweek_num'] = dfSuper['visit_date'].dt.dayofweek  # Monday is 0, Sunday is 6\n",
    "\n",
    "    # Weekend\n",
    "    dfSuper['weekend'] = dfSuper.apply(lambda row: fn_is_weekend(row['dayofweek_num']), axis=1)\n",
    "\n",
    "    # Correlations\n",
    "    # reservations to visitors correlation\n",
    "    dfCorr = dfSuper.copy()\n",
    "    # shorten the list\n",
    "    dfCorr = dfCorr[['air_store_id','visitors','reserve_visitors']]\n",
    "    # re-create dataframe from correlation grouped by the air_store_id\n",
    "    dfCorr = dfCorr.groupby('air_store_id')[['visitors','reserve_visitors']].corr().iloc[0::2]\n",
    "    # rename the colum\n",
    "    dfCorr = dfCorr.rename(columns={'reserve_visitors':'corr_vis_resv'})\n",
    "    # drop the \"visitors\" columns that are left over from the matrix\n",
    "    dfCorr.reset_index(inplace=True)\n",
    "    dfCorr.drop(['visitors','level_1'], axis=1, inplace=True)\n",
    "    # set any nan to 0\n",
    "    dfCorr['corr_vis_resv'].replace({np.nan: 0}, inplace=True)\n",
    "    # join back\n",
    "    dfSuper= pd.merge(dfSuper, dfCorr, how='left', on ='air_store_id', )\n",
    "    dfSuper.rename(columns={'corr_vis_resv_x':'corr_vis_resv'}, inplace=True)\n",
    "\n",
    "    \n",
    "    # Drop the duplicated columns\n",
    "    dfSuper.drop(['hpg_store_id','hpg_genre_name','hpg_area_name','hpg_latitude','hpg_longitude','air_genre_name',\n",
    "                   'air_area_name','air_latitude','air_longitude', 'hpg_reserve_visitors','air_reserve_visitors'], axis=1, inplace=True)\n",
    "\n",
    "    # Set the indexes for the data\n",
    "    # Set Indexes\n",
    "    # 01/13/2018 - Ben Grauer - Changed\n",
    "    dfSuper.set_index(['air_store_id','visit_date'], inplace=True)\n",
    "\n",
    "    # FUNCTION \n",
    "    # Add the weighte moving averages / etc\n",
    "    dfSuper = fn_ts_add_cycletrend_analysis(dfSuper, 'visitors')\n",
    "    \n",
    "    # Reset just to the visit date\n",
    "    dfSuper.reset_index(level=0, inplace=True)\n",
    "    \n",
    "    # Set any floats for the data (to work with time-series)\n",
    "    # Set data types\n",
    "    dfSuper['visitors'] = dfSuper['visitors'].astype('float32')\n",
    "\n",
    "    \n",
    "    # CONVERT DUMMY VARIABLES\n",
    "    if loadDummyVariables==1:\n",
    "        dfSuper = convert_dummy_vars(dataFrame=dfSuper, columnName='day_of_week')\n",
    "        dfSuper = convert_dummy_vars(dataFrame=dfSuper, columnName='month_name')\n",
    "        \n",
    "    \n",
    "    # Create columns and default (initialize)\n",
    "    dfSuper['forecast'] = 0\n",
    "    dfSuper['forecast'] = dfSuper['forecast'].astype('float32')    \n",
    "    \n",
    "    print (datetime.now() - startTime)\n",
    "    print('Data Load - Finished')\n",
    "     \n",
    "    # Final Re-Organization of columns\n",
    "    \n",
    "    return dfSuper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tired eyes - could have made this better to integrate with above, but it works\n",
    "#  Next comp - will join it together since I have some many scripts using the above\n",
    "def fn_load_test_dates(ben):\n",
    "    print(ben)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script is possibly locked right now\n",
    "def fn_load_test_dates(airStoreId, df):\n",
    "    #airStoreId = 'air_24e8414b9b07decb'\n",
    "    #df = dfBen.copy()\n",
    "    # set to the individual store\n",
    "    df = df[df['air_store_id'] == airStoreId]\n",
    "\n",
    "    # Grab items to impute\n",
    "    airStoreId = airStoreId\n",
    "    visitors = 0.0\n",
    "    genreName = df['genre_name'].head(1)[0]\n",
    "    areaName = df['area_name'].head(1)[0]\n",
    "    latitude = df['latitude'].head(1)[0]\n",
    "    longitude = df['longitude'].head(1)[0]\n",
    "\n",
    "    # add The test dates\n",
    "    idx = pd.DataFrame(pd.date_range('2017-04-23','2017-05-31'), columns={'dateRange'})\n",
    "    idx.set_index('dateRange',inplace=True)\n",
    "    df = pd.concat([df,idx], axis=1)\n",
    "\n",
    "    df = df[:][\"2017-04-23\":\"2017-05-31\"]\n",
    "\n",
    "    # join on the air-visit dates for holiday\n",
    "    # May need to reset-the inde\n",
    "    pd_date_info = pd.read_csv(workDir + 'date_info.csv', parse_dates=(['calendar_date']), infer_datetime_format=True)\n",
    "    pd_date_info.set_index('calendar_date', inplace=True)\n",
    "    pd_date_info = pd_date_info[:][\"2017-04-23\":\"2017-05-31\"]\n",
    "    #df = pd.merge(df.reset_index(), pd_date_info, how='inner', left_on='index', right_on='calendar_date')\n",
    "\n",
    "    df['calendar_date'] = pd_date_info.index\n",
    "    df['day_of_week'] = pd_date_info['day_of_week']\n",
    "    df['holiday_flg'] = pd_date_info['holiday_flg']\n",
    "\n",
    "    # Append features\n",
    "    df['month_num'] = df.index.month\n",
    "    df['month_name'] = df['month_num'].apply(lambda x: calendar.month_abbr[x])\n",
    "\n",
    "    df['dayofmonth_num'] = df.index.day\n",
    "    df['dayofweek_num'] = df.index.dayofweek  # Monday is 0, Sunday is 6\n",
    "\n",
    "    # Weekend\n",
    "    df['weekend'] = df.apply(lambda row: fn_is_weekend(row['dayofweek_num']), axis=1)\n",
    "\n",
    "    # Set Items\n",
    "    df['air_store_id'] = airStoreId\n",
    "    df['visitors'] = 0.0\n",
    "    df['genre_name'] = genreName\n",
    "    df['area_name'] = areaName\n",
    "    df['latitude'] = latitude\n",
    "    df['longitude'] = longitude\n",
    "    df.replace({np.nan: 0}, inplace=True)\n",
    "\n",
    "    # day of week + Month - the hard way!\n",
    "    df['Apr'][\"2017-04-23\":\"2017-04-30\"] = 1\n",
    "    df['May'][\"2017-05-01\":\"2017-05-31\"] = 1\n",
    "\n",
    "    #lambda x: True if x % 2 == 0 else False\n",
    "    df['Monday'] = df['dayofweek_num'].apply(lambda row: 1 if row==0 else 0 )\n",
    "    df['Tuesday'] = df['dayofweek_num'].apply(lambda row: 1 if row==1 else 0 )\n",
    "    df['Wednesday'] = df['dayofweek_num'].apply(lambda row: 1 if row==2 else 0 )\n",
    "    df['Thursday'] = df['dayofweek_num'].apply(lambda row: 1 if row==3 else 0 )\n",
    "    df['Friday'] = df['dayofweek_num'].apply(lambda row: 1 if row==4 else 0 )\n",
    "    df['Saturday'] = df['dayofweek_num'].apply(lambda row: 1 if row==5 else 0 )\n",
    "    df['Sunday'] = df['dayofweek_num'].apply(lambda row: 1 if row==6 else 0 )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONST_TEST_MODE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONST_TEST_MODE==1:\n",
    "    # TESTING\n",
    "    dfBen = fn_load_all_data(1).copy()\n",
    "    storeId = 'air_24e8414b9b07decb'\n",
    "    dfBen = fn_load_test_dates_and_add_attributes(storeId, dfBen)\n",
    "\n",
    "# now join on the function\n",
    "#dfBen = pd.concat([dfBen,fn_load_test_dates_and_add_attributes(storeId, dfBen)], axis=1)\n",
    "#dfBen.tail(8).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function above\n",
    "if CONST_TEST_MODE==1:\n",
    "    df = fn_load_all_data(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = fn_ts_add_cycletrend_analysis(df, 'visitors')\n",
    "#df[['visitors','7-day-SMA']].plot()\n",
    "if CONST_TEST_MODE==1:\n",
    "    df[['SMA_3_days','SMA_7_days']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONST_TEST_MODE==1:\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorgder the columns\n",
    "#df = df[['air_store_id','visitors','reserve_visitors','corr_vis_resv','genre_name','area_name','latitude','longitude',\\\n",
    "#    'calendar_date','holiday_flg','month_num','month_name','dayofmonth_num','day_of_week','dayofweek_num','weekend',\\\n",
    "#    'forecast']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do each one in a consolidated\n",
    "def convert_dummy_vars(dataFrame, columnName, dropFirst=False, colRename=''):\n",
    "    \n",
    "    temp = pd.get_dummies(dataFrame[columnName], drop_first=dropFirst)\n",
    "    \n",
    "    if colRename != '':\n",
    "        temp.columns = [(colRename)]\n",
    "    \n",
    "    #dataFrame.drop([columnName], axis=1, inplace=True)\n",
    "    dataFrame = pd.concat([dataFrame, temp], axis=1)    \n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nben = df.copy()\\nben.head()\\n\\n# CHOOSE WHICH FEATURES TO INCLUDE\\n# 'amount_tsh', - leave out - add after binning and plotting\\n\\nshortList_v1 = ['day_of_week','month_name']\\nben = convert_dummy_vars(dataFrame=ben, columnName='day_of_week')\\nben = convert_dummy_vars(dataFrame=ben, columnName='month_name')\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ben = df.copy()\n",
    "ben.head()\n",
    "\n",
    "# CHOOSE WHICH FEATURES TO INCLUDE\n",
    "# 'amount_tsh', - leave out - add after binning and plotting\n",
    "\n",
    "shortList_v1 = ['day_of_week','month_name']\n",
    "ben = convert_dummy_vars(dataFrame=ben, columnName='day_of_week')\n",
    "ben = convert_dummy_vars(dataFrame=ben, columnName='month_name')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resuming a file where it left off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "# Check if the file exists\n",
    "def fn_determine_file_exists(fileName):\n",
    "    fileExists = False\n",
    "\n",
    "    if path.isfile(fileName):\n",
    "        fileExists = True\n",
    "\n",
    "    return  fileExists\n",
    "\n",
    "# Will determine where the file / process left off to pick back up\n",
    "def fn_determine_file_last_run(fileName, colNames):\n",
    "\n",
    "    # Read In\n",
    "    df_leftOff = pd.read_csv(fileName)\n",
    "    \n",
    "    # Grab the unique store number \n",
    "    df_leftOff = pd.DataFrame(df_leftOff['air_store_id'].unique(), columns=(colNames))\n",
    "    \n",
    "    # if I re-use this later, need to handle for multiple column names (if applicable)\n",
    "    df_leftOff.sort_values(['air_store_id'], ascending=[True], inplace=True)\n",
    "\n",
    "    # Set a Processed Flag for all the entries\n",
    "    df_leftOff['processed'] = 1\n",
    "\n",
    "    # Return a data frame to join later\n",
    "    return df_leftOff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# NEED TO TURN THIS INTO FUNCTION\n",
    "# Call the function to determine where we left off\n",
    "if fn_determine_file_exists(exportResultsSubmissionFileName) == True:\n",
    "\n",
    "    # Repull\n",
    "    existingDF = fn_determine_file_last_run(exportResultsSubmissionFileName, ['air_store_id'])\n",
    "    print('Existing File detected with ' + str(len(existingDF)) + ' entries.'\n",
    "\n",
    "    # Join\n",
    "    resumeDF = pd.merge(rundf.reset_index(), existingDF, how='left', on='air_store_id', copy=True)\n",
    "    resumeDF = resumeDF[resumeDF['processed']==1].copy()\n",
    "    resumeDF.set_index('visit_date', inplace=True)\n",
    "    # re order\n",
    "    resumeDF.sort_values(['air_store_id'], ascending=[True], inplace=True)\n",
    "\n",
    "    # set to True for below\n",
    "    resumeRunningPreviousFile = True\n",
    "    print('Existing File Detected')\n",
    "    \n",
    "    # Set the runnign dataframe to the resumed one\n",
    "    rundf = df_resume.copy()\n",
    "\n",
    "# Determine if we include header (multiple booleans as I had split multiple output files earlier)\n",
    "if resumeRunningPreviousFile == True:\n",
    "    includeSubmissionHeaderRunOnce = False\n",
    "    \n",
    "# Re-Order - whether we are resuming or not\n",
    "rundf.sort_values(['air_store_id'], ascending=[True], inplace=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA / Plot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# FUNCTION\\ndef fn_ts_add_cycletrend_analysis(df, colName):\\n        \\n    # cycle and trend\\n    cycle, trend = sm.tsa.filters.hpfilter(df[colName])\\n    df['cycle'] = cycle\\n    df['trend'] = trend\\n    \\n    # Simple moving average\\n    df['3-day-SMA'] = df[colName].rolling(window=3).mean().replace({np.nan: 0}, inplace=True)\\n    df['7-day-SMA'] = df[colName].rolling(window=7).mean().replace({np.nan: 0}, inplace=True)\\n    df['14-day-SMA'] = df[colName].rolling(window=14).mean().replace({np.nan: 0}, inplace=True)\\n    df['31-day-SMA'] = df[colName].rolling(window=31).mean().replace({np.nan: 0}, inplace=True)\\n    \\n    # EWMA\\n    df['EWMA_3_days'] = df[colName].ewm(span=3).mean().replace({np.nan: 0}, inplace=True)\\n    df['EWMA_7_days'] = df[colName].ewm(span=7).mean().replace({np.nan: 0}, inplace=True)\\n    df['EWMA_14_days'] = df[colName].ewm(span=14).mean().replace({np.nan: 0}, inplace=True)\\n    df['EWMA_31_days'] = df[colName].ewm(span=31).mean().replace({np.nan: 0}, inplace=True)\\n    \\n    #newColList = ['cycle','trend','3-day-SMA','7-day-SMA','14-day-SMA','31-day-SMA','EWMA_7_days','EWMA_14_days','EWMA_31_days']\\n    \\n    return df #, newColList\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# FUNCTION\n",
    "def fn_ts_add_cycletrend_analysis(df, colName):\n",
    "        \n",
    "    # cycle and trend\n",
    "    cycle, trend = sm.tsa.filters.hpfilter(df[colName])\n",
    "    df['cycle'] = cycle\n",
    "    df['trend'] = trend\n",
    "    \n",
    "    # Simple moving average\n",
    "    df['3-day-SMA'] = df[colName].rolling(window=3).mean().replace({np.nan: 0}, inplace=True)\n",
    "    df['7-day-SMA'] = df[colName].rolling(window=7).mean().replace({np.nan: 0}, inplace=True)\n",
    "    df['14-day-SMA'] = df[colName].rolling(window=14).mean().replace({np.nan: 0}, inplace=True)\n",
    "    df['31-day-SMA'] = df[colName].rolling(window=31).mean().replace({np.nan: 0}, inplace=True)\n",
    "    \n",
    "    # EWMA\n",
    "    df['EWMA_3_days'] = df[colName].ewm(span=3).mean().replace({np.nan: 0}, inplace=True)\n",
    "    df['EWMA_7_days'] = df[colName].ewm(span=7).mean().replace({np.nan: 0}, inplace=True)\n",
    "    df['EWMA_14_days'] = df[colName].ewm(span=14).mean().replace({np.nan: 0}, inplace=True)\n",
    "    df['EWMA_31_days'] = df[colName].ewm(span=31).mean().replace({np.nan: 0}, inplace=True)\n",
    "    \n",
    "    #newColList = ['cycle','trend','3-day-SMA','7-day-SMA','14-day-SMA','31-day-SMA','EWMA_7_days','EWMA_14_days','EWMA_31_days']\n",
    "    \n",
    "    return df #, newColList\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "#df = fn_ts_add_cycletrend_analysis(df, 'visitors')\n",
    "#df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ad-fuller Check\n",
    "def fn_adf_check(time_series):\n",
    "    \"\"\"\n",
    "    Pass in a time series, returns ADF report\n",
    "    \"\"\"\n",
    "    result = adfuller(time_series)\n",
    "    print('Augmented Dickey-Fuller Test:')\n",
    "    labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n",
    "\n",
    "    for value,label in zip(result,labels):\n",
    "        print(label+' : '+str(value) )\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\")\n",
    "        return CONST_STATIONARY\n",
    "    else:\n",
    "        print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")\n",
    "        return CONST_NON_STATIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_plot_auto_corr(df):\n",
    "    autocorrelation_plot(df.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_plot_acf(df):\n",
    "    plot = plot_acf(df.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_plot_pacf(df):\n",
    "    plot = plot_pacf(df.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_plot_decomposition(series, frequency):    \n",
    "    decomposition1 = seasonal_decompose(series, freq=frequency)\n",
    "    fig = plt.figure()\n",
    "    fig = decomposition1.plot()\n",
    "    fig.set_size_inches(20,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_plot_result_diagnostics(model_results):\n",
    "    fPlot = model_results.plot_diagnostics(figsize=(10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONST_TEST_MODE==1:\n",
    "    subDF = df[df['air_store_id']=='air_1c0b150f9e696a5f']\n",
    "    \n",
    "    fn_plot_auto_corr(subDF['visitors'])\n",
    "    fn_plot_acf(subDF['visitors'])\n",
    "    fn_plot_pacf(subDF['visitors'])\n",
    "    fn_plot_decomposition(subDF['visitors'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models to Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not yet tested / implemented in main script for now\n",
    "def fn_run_arima_timeseries(visitMergeDF, storeToEvaluate, orderList, logger):\n",
    "    \n",
    "    errorOccured = False\n",
    "    \n",
    "    if GLOBAL_CONST_DEBUG ==1:\n",
    "        print('Start Processing Restaraunt: ' + str(storeToEvaluate))\n",
    "    \n",
    "    # create ts sub-set\n",
    "    ts = visitMergeDF[visitMergeDF['air_store_id']==storeToEvaluate].copy()\n",
    "    ts.asfreq('D')\n",
    "    \n",
    "    minDate = ts.index.min()\n",
    "    \n",
    "    # add dates to predict\n",
    "    idx = pd.DataFrame(pd.date_range('2017-04-23','2017-05-31'), columns={'dateRange'})\n",
    "    idx.set_index('dateRange',inplace=True)\n",
    "    ts = pd.concat([ts,idx], axis=1)\n",
    "\n",
    "    # Impute\n",
    "    ts['air_store_id'].replace({np.nan: storeToEvaluate}, inplace=True)\n",
    "    ts['visitors'].replace({np.nan: 0}, inplace=True)\n",
    "    ts['visitors_log'].replace({np.nan: 0}, inplace=True)\n",
    "    \n",
    "    # Try Catch Here\n",
    "    try:\n",
    "    \n",
    "        # standard ARIMA model with order list passed in\n",
    "        model = ARIMA(ts['visitors'][minDate:\"2017-04-22\"], order=orderList)\n",
    "        modelFit = model.fit(disp=-1)\n",
    "        # 39 days is how far out we are predicting\n",
    "        results = modelFit.forecast(39)  \n",
    "        # set forecast\n",
    "        ts['forecast'][\"2017-04-23\":\"2017-05-31\"] = results[0][:]\n",
    "\n",
    "        if GLOBAL_CONST_DEBUG ==1:\n",
    "            print('Start Processing Restaraunt - LOG: ' + str(storeToEvaluate))\n",
    "\n",
    "        # Do the log while we are in here\n",
    "        modelLog = ARIMA(ts['visitors_log'][minDate:\"2017-04-22\"], order=orderList)\n",
    "        modelLog_fit = modelLog.fit(disp=-1)\n",
    "        resultsLog = modelLog_fit.forecast(39)\n",
    "        ts['forecast_log'][\"2017-04-23\":\"2017-05-31\"] = resultsLog[0][:]\n",
    "        ts['forecast_logExp'] = np.exp(ts['forecast_log'])  # Revert log back to standard\n",
    "    \n",
    "    \n",
    "    except: \n",
    "        #(RuntimeError, TypeError, NameError):\n",
    "        #print('Error')\n",
    "        #print(RuntimeError)\n",
    "        #print(TypeError)\n",
    "        #print(NameError)\n",
    "        #print('\\n')\n",
    "        logger.info('Error processing store: ' + storeToEvaluate)\n",
    "        errorOccured = True\n",
    "           \n",
    "    pass\n",
    "\n",
    "\n",
    "    if GLOBAL_CONST_DEBUG ==1:\n",
    "        print('Finished Processing Restaraunt: ' + str(storeToEvaluate))\n",
    "\n",
    "\n",
    "    # return back only what we predicted\n",
    "    dfTSReturn = ts[:][\"2017-04-23\":]\n",
    "    return dfTSReturn, errorOccured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_ARIMA_walk_forward(dfTS, colName, isTest, param):\n",
    "\n",
    "    # Make a step forward attempt - Practice from Page 215 from Jason's book (Adobe page 228)\n",
    "    minDate = testTS.index.min()\n",
    "\n",
    "    # We could pass in visitors or moving average\n",
    "    X = pd.Series(dfTS[colName][minDate:\"2017-04-22\"])\n",
    "\n",
    "    if isTest==1:\n",
    "        # grab 2/3 for testing\n",
    "        size = int(len(X) * 0.66)\n",
    "        train,test = X[0:size], X[size:len(X)]\n",
    "    else:\n",
    "        \n",
    "        # if we running live - Need 40 time setps out\n",
    "        train,test = X[0:len(X)-40], X[len(X)-39:len(X)]\n",
    "\n",
    "    # Keep track of history + predictions\n",
    "    history = [x for x in train]\n",
    "    predictions = list()\n",
    "\n",
    "    print('Total Test items: ' + str(len(test)))\n",
    "    \n",
    "    # Walk forward validation\n",
    "    for t in range(len(test)):\n",
    "        model = ARIMA(history, order=param)\n",
    "        model_fit = model.fit(disp=0)\n",
    "        output = model_fit.forecast()\n",
    "        \n",
    "        yhat = output[0]\n",
    "        predictions.append(yhat)\n",
    "        obs = test[t]\n",
    "        history.append(obs)\n",
    "        \n",
    "        # only print if test\n",
    "        if isTest==1:\n",
    "            if t%20==0:\n",
    "                print('Current Index: ' + str(t) + '.  predicted=%f, expected=%f' % (yhat, obs))\n",
    "            \n",
    "    return rmse, test, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_ts_add_test_dates(storeToEvaluate, df):\n",
    "    idx = pd.DataFrame(pd.date_range('2017-04-23','2017-05-31'), columns={'dateRange'})\n",
    "    idx.set_index('dateRange',inplace=True)\n",
    "    df = pd.concat([df,idx], axis=1)\n",
    "\n",
    "    # Impute\n",
    "    df['air_store_id'].replace({np.nan: storeToEvaluate}, inplace=True)\n",
    "    df['visitors'].replace({np.nan: 0}, inplace=True)\n",
    "    if 'visitors_log' in df:\n",
    "        df['visitors_log'].replace({np.nan: 0}, inplace=True)\n",
    "    \n",
    "    df['SMA_3_days'].replace({np.nan: 0.0}, inplace=True)\n",
    "    df['SMA_7_days'].replace({np.nan: 0.0}, inplace=True)\n",
    "    df['SMA_14_days'].replace({np.nan: 0.0}, inplace=True)\n",
    "    df['SMA_30_days'].replace({np.nan: 0.0}, inplace=True)\n",
    "    df['SMA_90_days'].replace({np.nan: 0.0}, inplace=True)\n",
    "\n",
    "    # EWMA\n",
    "    df['EWMA_3_days'].replace({np.nan: 0.0}, inplace=True)\n",
    "    df['EWMA_7_days'].replace({np.nan: 0.0}, inplace=True)\n",
    "    df['EWMA_14_days'].replace({np.nan: 0.0}, inplace=True)\n",
    "    df['EWMA_30_days'].replace({np.nan: 0.0}, inplace=True)\n",
    "    df['EWMA_90_days'].replace({np.nan: 0.0}, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict starts at 233 / Chap 27 and 28\n",
    "def fn_ARIMA_walk_forward_multistep(df, colName, param, daysCycle, size, minDateOverride):\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    errorOccured = False\n",
    "    \n",
    "    startTime = datetime.now()\n",
    "    minDate = df.index.min()\n",
    "    \n",
    "    # add an over-ride if I want to start later\n",
    "    if minDateOverride != '':\n",
    "        minDate = minDateOverride    \n",
    "    \n",
    "    # extend out the dataset\n",
    "    #df = fn_ts_add_test_dates(storeToEvaluate, df)\n",
    "    maxDate = df.index.max()\n",
    "    #print('max date: ' + str(maxDate))\n",
    "\n",
    "    X = pd.Series(df[colName][minDate:\"2017-05-31\"])\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        # Assign the start/finish to be used below\n",
    "        trainStart = 0\n",
    "        trainFinish = len(X)-size\n",
    "        testStart = len(X)-size\n",
    "        testFinish = len(X)\n",
    "        predictOutSize = daysCycle\n",
    "\n",
    "        train,test = X[trainStart:trainFinish], X[testStart:testFinish]\n",
    "        #print('test start: ' + str(testStart) + '.  test finish: ' + str(testFinish))\n",
    "\n",
    "        # Keep track of history + predictions\n",
    "        history = [x for x in train]\n",
    "        predictions = list()\n",
    "\n",
    "        #print('Total Test items: ' + str(len(test)))\n",
    "\n",
    "        forecast = 0\n",
    "\n",
    "        # Walk forward validation\n",
    "        for t in range(0, len(test), daysCycle):\n",
    "\n",
    "            # Later may need to play around with this\n",
    "            model = ARIMA(history, order=param)\n",
    "            model_fit = model.fit(disp=0)\n",
    "\n",
    "            # convert the x mount of days forecasted out to a list\n",
    "            yhat = list(model_fit.forecast(daysCycle)[0])\n",
    "            #predictions.append(yhat)\n",
    "\n",
    "            # add the list so it is all even to append at the end\n",
    "            predictions = predictions + yhat\n",
    "            obs = test[t]\n",
    "            history.append(obs)\n",
    "\n",
    "            #if t%size==0:\n",
    "                #print('Current Index: ' + str(t)) # + '.  predicted=%f, expected=%f' % (yhat, obs))\n",
    "\n",
    "        # Assign the fore-cast predictions\n",
    "        df['forecast'] = 0\n",
    "        df['forecast'][len(df)-size:len(df)] = predictions\n",
    "    \n",
    "    except: \n",
    "        errorOccured = True\n",
    "    pass\n",
    "        \n",
    "    # Print when it finished\n",
    "    #print ('Time to Finish: ' + str(datetime.now() - startTime)) \n",
    "    \n",
    "    # Return (dataframe, rmse(no longer applied in step) - calced outside of cuntion\n",
    "    return df[\"2017-04-23\":\"2017-05-31\"], errorOccured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalization of Submission Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will finalize the submission file\n",
    "def fn_finalize_submission_file(exportDir, exportFileName, colNameToSubmit):\n",
    "    \n",
    "    arimaExportDir = 'D:\\\\project\\\\data\\\\kg_jpn_rest\\\\export\\\\'\n",
    "    arimaExportFile = exportFileName + '.csv' # 'export_results.csv'\n",
    "\n",
    "    armRsltDF = pd.read_csv(arimaExportDir + arimaExportFile)\n",
    "    pd_sample_submission = pd.read_csv('D:\\\\project\\\\data\\\\kg_jpn_rest\\\\' + 'sample_submission.csv')\n",
    "    \n",
    "    # Concatenate the fields to create the ID\n",
    "    armRsltDF['id'] = ''\n",
    "    armRsltDF['id'] = armRsltDF['id'].str.cat(armRsltDF['air_store_id'])\n",
    "    armRsltDF['id'] = armRsltDF['id'] + '_'\n",
    "    armRsltDF['id'] = armRsltDF['id'].str.cat(armRsltDF['visit_date'].astype(str))\n",
    "\n",
    "    # Drop the visitors (will rename later)\n",
    "    armRsltDF.drop('visitors', axis=1, inplace=True)\n",
    "\n",
    "    # Must turn any negatives into a zero\n",
    "    # Not sure why this needs to come after, perhaps with the merge it throws some things between float/int offf\n",
    "    def fn_set_negative_to_zero(col):\n",
    "        if col < 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return col\n",
    "\n",
    "    # Took out the log items for now    \n",
    "    armRsltDF['forecast'] = armRsltDF.apply(lambda row: fn_set_negative_to_zero(row['forecast']), axis=1)\n",
    "    armRsltDF[armRsltDF['forecast']<0]['forecast'] = 0.0\n",
    "\n",
    "    # Join to sample submission\n",
    "    armRsltDF = pd.merge(pd_sample_submission, armRsltDF, how='inner', on=('id'))\n",
    "\n",
    "    # Drop the visitors (will rename later)\n",
    "    armRsltDF.drop('visitors', axis=1, inplace=True)\n",
    "\n",
    "    # 32019 - correct length\n",
    "    if (len(armRsltDF)) == 32019:\n",
    "        print('Correct Submission Length')\n",
    "    else:\n",
    "        print('!! ERROR !! - Incorrect Submission Length')\n",
    "\n",
    "    def fn_write_submission_file(df, colNames, fileName):\n",
    "\n",
    "        print(colNames[1])\n",
    "        forecastCol = colNames[1]\n",
    "\n",
    "        tempDF = df.copy()\n",
    "\n",
    "        # Rename the second column to \"visitors\" as per submission\n",
    "        tempDF.rename(columns={forecastCol: 'visitors'}, inplace=True)\n",
    "\n",
    "        #print(tempDF.head())\n",
    "        tempDF.to_csv(fileName, header=True, index=False, quotechar='\"', columns=('id','visitors'))\n",
    "        print('Wrote file: ' + fileName)\n",
    "\n",
    "    todayDate = str(dt.date.today().strftime('%Y%m%d'))\n",
    "\n",
    "    # Float forecast\n",
    "    exportDF = armRsltDF.copy()\n",
    "    exportDF\n",
    "    columns=('id','forecast')\n",
    "    #fn_write_submission_file(armRsltDF, columns, arimaExportDir + todayDate + '_subm_frcst_flt.csv')\n",
    "    fn_write_submission_file(armRsltDF, columns, arimaExportDir + todayDate + '_subm_' + exportFileName + '.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
