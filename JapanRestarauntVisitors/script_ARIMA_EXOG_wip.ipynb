{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import platform\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "import datetime as dt\n",
    "\n",
    "# Multiple dataframes in single cell\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working Dir / Load Data\n",
    "from sys import platform\n",
    "# if platform.system() == 'Windows':\n",
    "if platform == 'win32':\n",
    "    directory = 'D:\\\\project\\\\data\\\\kg_jpn_rest\\\\'\n",
    "    exportDirectory = directory + 'export\\\\'\n",
    "\n",
    "# Mac\n",
    "#elif platform.system() == 'Darwin':\n",
    "elif platform == 'darwin':\n",
    "    directory = '//Project/data/kg_corpgroc/'\n",
    "    exportDirectory = directory + 'export/'\n",
    "\n",
    "# AWS\n",
    "elif platform == 'linux':\n",
    "    directory = '//data/'\n",
    "    exportDirectory = directory + 'export/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to load notebooks borrowed from online\n",
    "# http://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Importing%20Notebooks.html\n",
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "\n",
    "def find_notebook(fullname, path=None):\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        \n",
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "    \n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "        \n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "                                       \n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = read(f, 4)\n",
    "        \n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "        \n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "        \n",
    "        try:\n",
    "          for cell in nb.cells:\n",
    "            if cell.cell_type == 'code':\n",
    "                # transform the input to executable Python\n",
    "                code = self.shell.input_transformer_manager.transform_cell(cell.source)\n",
    "                # run the code in themodule\n",
    "                exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "    \n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "\n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "\n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "\n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]   \n",
    "    \n",
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from helper_notebook.ipynb\n"
     ]
    }
   ],
   "source": [
    "import helper_notebook as hlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hlp.fn_finalize_submission_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Load - Loading data on: Windows Directory: D:\\project\\data\\kg_jpn_rest\\\n",
      "Data Load - Merging Data\n",
      "Data Load - Adding Features\n",
      "Data Load - Finished\n"
     ]
    }
   ],
   "source": [
    "# Load the data set / cleaned, joined, and formatted\n",
    "dfSuper = hlp.fn_load_all_data(1)\n",
    "dfSuper, newColList = hlp.fn_ts_add_cycletrend_analysis(dfSuper, 'visitors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_store_id</th>\n",
       "      <th>visitors</th>\n",
       "      <th>calendar_date</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>holiday_flg</th>\n",
       "      <th>genre_name</th>\n",
       "      <th>area_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>reserve_visitors</th>\n",
       "      <th>...</th>\n",
       "      <th>forecast</th>\n",
       "      <th>cycle</th>\n",
       "      <th>trend</th>\n",
       "      <th>3-day-SMA</th>\n",
       "      <th>7-day-SMA</th>\n",
       "      <th>14-day-SMA</th>\n",
       "      <th>31-day-SMA</th>\n",
       "      <th>EWMA_7_days</th>\n",
       "      <th>EWMA_14_days</th>\n",
       "      <th>EWMA_31_days</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visit_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-13</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>Dining bar</td>\n",
       "      <td>Tōkyō-to Minato-ku Shibakōen</td>\n",
       "      <td>35.658068</td>\n",
       "      <td>139.751599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.713874</td>\n",
       "      <td>23.286126</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-14</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2016-01-14</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>0</td>\n",
       "      <td>Dining bar</td>\n",
       "      <td>Tōkyō-to Minato-ku Shibakōen</td>\n",
       "      <td>35.658068</td>\n",
       "      <td>139.751599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.107589</td>\n",
       "      <td>22.892411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>28.750000</td>\n",
       "      <td>28.612903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-15</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2016-01-15</td>\n",
       "      <td>Friday</td>\n",
       "      <td>0</td>\n",
       "      <td>Dining bar</td>\n",
       "      <td>Tōkyō-to Minato-ku Shibakōen</td>\n",
       "      <td>35.658068</td>\n",
       "      <td>139.751599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.500232</td>\n",
       "      <td>22.499768</td>\n",
       "      <td>28.666667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>28.845501</td>\n",
       "      <td>28.750347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-16</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2016-01-16</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>0</td>\n",
       "      <td>Dining bar</td>\n",
       "      <td>Tōkyō-to Minato-ku Shibakōen</td>\n",
       "      <td>35.658068</td>\n",
       "      <td>139.751599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.114960</td>\n",
       "      <td>22.114960</td>\n",
       "      <td>27.666667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.440000</td>\n",
       "      <td>26.751269</td>\n",
       "      <td>26.896050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-18</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2016-01-18</td>\n",
       "      <td>Monday</td>\n",
       "      <td>0</td>\n",
       "      <td>Dining bar</td>\n",
       "      <td>Tōkyō-to Minato-ku Shibakōen</td>\n",
       "      <td>35.658068</td>\n",
       "      <td>139.751599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.748812</td>\n",
       "      <td>21.748812</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.740077</td>\n",
       "      <td>21.337295</td>\n",
       "      <td>22.160784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    air_store_id  visitors calendar_date day_of_week  \\\n",
       "visit_date                                                             \n",
       "2016-01-13  air_ba937bf13d40fb24      25.0    2016-01-13   Wednesday   \n",
       "2016-01-14  air_ba937bf13d40fb24      32.0    2016-01-14    Thursday   \n",
       "2016-01-15  air_ba937bf13d40fb24      29.0    2016-01-15      Friday   \n",
       "2016-01-16  air_ba937bf13d40fb24      22.0    2016-01-16    Saturday   \n",
       "2016-01-18  air_ba937bf13d40fb24       6.0    2016-01-18      Monday   \n",
       "\n",
       "            holiday_flg genre_name\\t                     area_name   latitude  \\\n",
       "visit_date                                                                      \n",
       "2016-01-13            0   Dining bar  Tōkyō-to Minato-ku Shibakōen  35.658068   \n",
       "2016-01-14            0   Dining bar  Tōkyō-to Minato-ku Shibakōen  35.658068   \n",
       "2016-01-15            0   Dining bar  Tōkyō-to Minato-ku Shibakōen  35.658068   \n",
       "2016-01-16            0   Dining bar  Tōkyō-to Minato-ku Shibakōen  35.658068   \n",
       "2016-01-18            0   Dining bar  Tōkyō-to Minato-ku Shibakōen  35.658068   \n",
       "\n",
       "             longitude  reserve_visitors      ...       forecast      cycle  \\\n",
       "visit_date                                    ...                             \n",
       "2016-01-13  139.751599               0.0      ...            0.0   1.713874   \n",
       "2016-01-14  139.751599               0.0      ...            0.0   9.107589   \n",
       "2016-01-15  139.751599               0.0      ...            0.0   6.500232   \n",
       "2016-01-16  139.751599               0.0      ...            0.0  -0.114960   \n",
       "2016-01-18  139.751599               0.0      ...            0.0 -15.748812   \n",
       "\n",
       "                trend  3-day-SMA  7-day-SMA  14-day-SMA  31-day-SMA  \\\n",
       "visit_date                                                            \n",
       "2016-01-13  23.286126        NaN        NaN         NaN         NaN   \n",
       "2016-01-14  22.892411        NaN        NaN         NaN         NaN   \n",
       "2016-01-15  22.499768  28.666667        NaN         NaN         NaN   \n",
       "2016-01-16  22.114960  27.666667        NaN         NaN         NaN   \n",
       "2016-01-18  21.748812  19.000000        NaN         NaN         NaN   \n",
       "\n",
       "            EWMA_7_days  EWMA_14_days  EWMA_31_days  \n",
       "visit_date                                           \n",
       "2016-01-13    25.000000     25.000000     25.000000  \n",
       "2016-01-14    29.000000     28.750000     28.612903  \n",
       "2016-01-15    29.000000     28.845501     28.750347  \n",
       "2016-01-16    26.440000     26.751269     26.896050  \n",
       "2016-01-18    19.740077     21.337295     22.160784  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSuper.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code Block Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\project\\data\\kg_jpn_rest\\export\\\n"
     ]
    }
   ],
   "source": [
    "# Variable to specify if we include a header one time in the files\n",
    "includeHeaderRunOnce = True\n",
    "# Determine if we are resuming a previous file\n",
    "resumeRunningPreviousFile = False\n",
    "\n",
    "\n",
    "print(exportDirectory)\n",
    "    \n",
    "#exportParamOptionsFileName = exportDirectory + 'export_param_' + str(file_args_store_nbr) + '.csv'\n",
    "exportResultsSubmissionFileName = exportDirectory + 'export_results.csv'\n",
    "\n",
    "exportLogName = exportDirectory + 'export_log.log'\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# SETUP LOGGING\n",
    "# ===========================\n",
    "# Wipe any existing log file - change to keep this script\n",
    "#if path.isfile(exportLogName):\n",
    "#    os.remove(exportLogName)\n",
    "\n",
    "# We may set another parameter to pass in to wipe the existing param options and results submissions\n",
    "\n",
    "# SEt logging information\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# create a file handler\n",
    "handler = logging.FileHandler(exportLogName)\n",
    "handler.setLevel(logging.INFO)\n",
    "\n",
    "# create a logging format\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "# add the handlers to the logger\n",
    "logger.addHandler(handler)\n",
    "logger.info('Start Logging')\n",
    "\n",
    "\n",
    "# Find out if we have an existing file and work with it from there to update the mergedDataFrame\n",
    "includeSubmissionHeaderRunOnce = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if we are resuming existing File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "# Check if the file exists\n",
    "def fn_determine_file_exists(fileName):\n",
    "    fileExists = False\n",
    "\n",
    "    if path.isfile(fileName):\n",
    "        fileExists = True\n",
    "\n",
    "    return  fileExists\n",
    "\n",
    "# Will determine where the file / process left off to pick back up\n",
    "def fn_determine_file_last_run(fileName, colNames):\n",
    "\n",
    "    # Read In\n",
    "    df_leftOff = pd.read_csv(fileName)\n",
    "    \n",
    "    # Grab the unique store number \n",
    "    df_leftOff = pd.DataFrame(df_leftOff['air_store_id'].unique(), columns=(colNames))\n",
    "    \n",
    "    \n",
    "    # if I re-use this later, need to handle for multiple column names (if applicable)\n",
    "    df_leftOff.sort_values(['air_store_id'], ascending=[True], inplace=True)\n",
    "\n",
    "    # Set a Processed Flag for all the entries\n",
    "    df_leftOff['processed'] = 1\n",
    "\n",
    "    # Return a data frame to join later\n",
    "    return df_leftOff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!! THIS IS STILL BROKEN - NEED TO ADJUST / FIX - Merging too many records\n",
    "# Determine if we are resuming a file\n",
    "# this could stand to be cleaned up.  \n",
    "\n",
    "# Set a default running dataframe - to be used to grab unique values\n",
    "rundf = dfSuper.copy()\n",
    "\n",
    "# Call the function to determine where we left off\n",
    "if fn_determine_file_exists(exportResultsSubmissionFileName) == True:\n",
    "\n",
    "    # Grab the completed results\n",
    "    #completed_df = fn_determine_file_last_run(exportResultsSubmissionFileName, ['air_store_id'])\n",
    "    existingDF = fn_determine_file_last_run(exportResultsSubmissionFileName, ['air_store_id'])\n",
    "    \n",
    "    # Should join back to test.  If there is no records, then we will scoop in the end. (meaning skip over).\n",
    "    # feel this is a bit over-complicated, but late nights\n",
    "    \n",
    "    # Make a copy of the visit merged training\n",
    "    copyTrainDF = visitMergeDF.copy(deep=True)\n",
    "    # reset index to keep the visit date\n",
    "    copyTrainDF.reset_index(inplace=True)\n",
    "    # merge the train to the existing df to get the processed\n",
    "    df_resume = pd.merge(copyTrainDF, existingDF, how='left', on='air_store_id', copy=True)\n",
    "    # Reset the index\n",
    "    df_resume.set_index('visit_date', inplace=True)\n",
    "    # set all blank unprocssed to 0 to mean process\n",
    "    df_resume['processed'].replace({np.nan: 0}, inplace=True)\n",
    "\n",
    "    # grab only the unprocessed\n",
    "    df_resume = df_resume[df_resume['processed']==0]\n",
    "    \n",
    "    # Resort anyway\n",
    "    df_resume.sort_values(['air_store_id'], ascending=[True], inplace=True)\n",
    "\n",
    "    # Then filter down the data-set to only un-filtered\n",
    "    #df_test_iteration = df_resume[df_resume['processed']==0].copy()\n",
    "\n",
    "    # set to True for below\n",
    "    resumeRunningPreviousFile = True\n",
    "    print('Existing File Detected')\n",
    "    \n",
    "    # Set the runnign dataframe to the resumed one\n",
    "    rundf = df_resume\n",
    "\n",
    "# Determine if we include header (multiple booleans as I had split multiple output files earlier)\n",
    "if resumeRunningPreviousFile == True:\n",
    "    includeSubmissionHeaderRunOnce = False\n",
    "    \n",
    "# Re-Order - in case we are not resuming\n",
    "rundf.sort_values(['air_store_id'], ascending=[True], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through all records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHERE THE FUNCTION WOULD BE\n",
    "def fn_ts_forecast_ARIMA_exog(df, storeId, exogParam, orderParam, visitCorrThreshold, shift, plot):\n",
    "    warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "    errorOccured = False\n",
    "    \n",
    "    # Set the storeId\n",
    "    ts = df[df['air_store_id']==storeId].copy()\n",
    "    ts.asfreq('D')  # not needed\n",
    "    ts['forecast'] = 0 # initialize\n",
    "\n",
    "    visitorCorr = ts['corr_vis_resv'].head(1)\n",
    "    \n",
    "    # If correlation is above a threshold, then add the exog parameter of reservations for visitors\n",
    "    if float(visitorCorr) >= float(visitCorrThreshold):\n",
    "        #print('Visitor Corr: ' + str(visitorCorr))\n",
    "        exogParam = exogParam + ['reserve_visitors']\n",
    "    \n",
    "    minDate = ts.index.min()\n",
    "    maxDate = ts.index.max() \n",
    "    maxDate = maxDate - pd.DateOffset(30) # offset 30 days to see how the model will perform on train/test data\n",
    "    #print('max date: ' + str(maxDate))\n",
    "    \n",
    "    forecast_start_range = len(ts) - 30  # so let's play with a month out to start the prediction\n",
    "    # second try would be 15\n",
    "    forecast_out_range = len(ts) + 39  # this is the number of days between 2017-04-22 and 2017-05-31\n",
    "    # It needs + 39 to properly forecast\n",
    "\n",
    "    # Add forecast range\n",
    "    idx = pd.DataFrame(pd.date_range('2017-04-23','2017-05-31'), columns={'dateRange'})\n",
    "    idx.set_index('dateRange',inplace=True)\n",
    "    ts = pd.concat([ts,idx], axis=1)\n",
    "\n",
    "    # Impute\n",
    "    ts['air_store_id'].replace({np.nan: storeId}, inplace=True)\n",
    "    ts['visitors'].replace({np.nan: 0}, inplace=True)\n",
    "    ts.replace({np.nan: 0}, inplace=True)\n",
    "    ts['forecast'] = 0.0\n",
    "\n",
    "    # Try Catch Here\n",
    "    try:\n",
    "    \n",
    "        #minDate = '2017-01-01'\n",
    "        model = sm.tsa.ARIMA(endog=ts['visitors'][minDate:maxDate], exog=ts[exogParam][minDate:maxDate], order=orderParam)\n",
    "        modelFit = model.fit()\n",
    "        modelPred = modelFit.predict(start=forecast_start_range, end=forecast_out_range, exog=ts[exogParam])  # , dynamic=True\n",
    "        #print('length of model: ' + str(len(modelPred)))  #70\n",
    "\n",
    "        lengthModelPred = len(modelPred)\n",
    "\n",
    "        #ts['forecast'][\"2017-03-18\":\"2017-05-31\"] = modelPred[:]\n",
    "        ts['forecast'][len(ts)-lengthModelPred : len(ts)] = modelPred[:]\n",
    "        if shift == 1:\n",
    "            ts['forecast'] = (ts['forecast'].shift(+1))\n",
    "\n",
    "        # Grab AIC and MSE\n",
    "        y = ts['visitors'][\"2017-03-23\":\"2017-04-22\"]\n",
    "        y_frcast = ts['forecast'][\"2017-03-23\":\"2017-04-22\"]\n",
    "        mse_frcast = ((y_frcast - y) ** 2).mean()\n",
    "        #print('AIC: ' + str(modelFit.aic))\n",
    "        #print('MSE: ' + str(mse_frcast))\n",
    "        #print('\\n')\n",
    "    \n",
    "\n",
    "    except:\n",
    "        #(RuntimeError, TypeError, NameError, ValueError): \n",
    "        #errorOccured = True\n",
    "        #(RuntimeError, TypeError, NameError):\n",
    "        #print('Error')\n",
    "        #print(RuntimeError())\n",
    "        #print(TypeError)\n",
    "        #print(NameError)\n",
    "        #print(ValueError)\n",
    "        #print('\\n')\n",
    "        logger.info('Error processing store: ' + storeId)\n",
    "        errorOccured = True        \n",
    "    pass\n",
    "\n",
    "\n",
    "    \n",
    "    #newColList = ['cycle','trend','3-day-SMA',,'31-day-SMA',,'EWMA_31_days']\n",
    "    plotColList = ['visitors','forecast','reserve_visitors','holiday_flg','weekend']\n",
    "    plotColList = plotColList + ['7-day-SMA','EWMA_7_days']\n",
    "    \n",
    "    if plot==1:\n",
    "        ts[plotColList][\"2016-01-01\":\"2017-05-31\"].plot(grid=True)\n",
    "    \n",
    "    return ts[:][\"2017-04-23\":], errorOccured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHERE THE FUNCTION WOULD BE\n",
    "def fn_ts_forecast_ARIMA_exog2(df, storeId, colName, exogParam, orderParam,\n",
    "                               visitCorrThreshold, shift, plot):\n",
    "    warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "    errorOccured = False\n",
    "    \n",
    "    # Set the storeId\n",
    "    ts = df[df['air_store_id']==storeId].copy()\n",
    "    ts.asfreq('D')  # not needed\n",
    "    ts['forecast'] = 0 # initialize\n",
    "\n",
    "    visitorCorr = ts['corr_vis_resv'].head(1)\n",
    "    \n",
    "    # If correlation is above a threshold, then add the exog parameter of reservations for visitors\n",
    "    if float(visitorCorr) >= float(visitCorrThreshold):\n",
    "        #print('Visitor Corr: ' + str(visitorCorr))\n",
    "        exogParam = exogParam + ['reserve_visitors']\n",
    "    \n",
    "    minDate = ts.index.min()\n",
    "    maxDate = ts.index.max() \n",
    "    maxDate = maxDate - pd.DateOffset(30) # offset 30 days to see how the model will perform on train/test data\n",
    "    #print('max date: ' + str(maxDate))\n",
    "    \n",
    "    forecast_start_range = len(ts) - 60  # so let's play with a month out to start the prediction\n",
    "    # second try would be 15\n",
    "    forecast_out_range = len(ts) + 39  # this is the number of days between 2017-04-22 and 2017-05-31\n",
    "    # It needs + 39 to properly forecast\n",
    "\n",
    "    # Add forecast range\n",
    "    idx = pd.DataFrame(pd.date_range('2017-04-23','2017-05-31'), columns={'dateRange'})\n",
    "    idx.set_index('dateRange',inplace=True)\n",
    "    ts = pd.concat([ts,idx], axis=1)\n",
    "\n",
    "    # Impute\n",
    "    ts['air_store_id'].replace({np.nan: storeId}, inplace=True)\n",
    "    ts[colName].replace({np.nan: 0}, inplace=True)\n",
    "    ts.replace({np.nan: 0}, inplace=True)\n",
    "    ts['forecast'] = 0.0\n",
    "\n",
    "    # Try Catch Here\n",
    "    try:\n",
    "    \n",
    "        #minDate = '2017-01-01'\n",
    "        model = sm.tsa.ARIMA(endog=ts[colName][minDate:maxDate], exog=ts[exogParam][minDate:maxDate], order=orderParam)\n",
    "        modelFit = model.fit()\n",
    "        modelPred = modelFit.predict(start=forecast_start_range, end=forecast_out_range, exog=ts[exogParam])  # , dynamic=True\n",
    "        #print('length of model: ' + str(len(modelPred)))  #70\n",
    "\n",
    "        lengthModelPred = len(modelPred)\n",
    "\n",
    "        #ts['forecast'][\"2017-03-18\":\"2017-05-31\"] = modelPred[:]\n",
    "        ts['forecast'][len(ts)-lengthModelPred : len(ts)] = modelPred[:]\n",
    "        if shift == 1:\n",
    "            ts['forecast'] = (ts['forecast'].shift(+1))\n",
    "\n",
    "        # Grab AIC and MSE\n",
    "        y = ts['visitors'][\"2017-03-23\":\"2017-04-22\"]\n",
    "        y_frcast = ts['forecast'][\"2017-03-23\":\"2017-04-22\"]\n",
    "        mse_frcast = ((y_frcast - y) ** 2).mean()\n",
    "        #print('AIC: ' + str(modelFit.aic))\n",
    "        #print('MSE: ' + str(mse_frcast))\n",
    "        #print('\\n')\n",
    "    \n",
    "\n",
    "    except:\n",
    "        #(RuntimeError, TypeError, NameError, ValueError): \n",
    "        #errorOccured = True\n",
    "        #(RuntimeError, TypeError, NameError):\n",
    "        #print('Error')\n",
    "        #print(RuntimeError())\n",
    "        #print(TypeError)\n",
    "        #print(NameError)\n",
    "        #print(ValueError)\n",
    "        #print('\\n')\n",
    "        logger.info('Error processing store: ' + storeId)\n",
    "        errorOccured = True        \n",
    "    pass\n",
    "    \n",
    "    #newColList = ['cycle','trend','3-day-SMA',,'31-day-SMA',,'EWMA_31_days']\n",
    "    plotColList = ['visitors','forecast','reserve_visitors','holiday_flg','weekend']\n",
    "    plotColList = plotColList + ['7-day-SMA','EWMA_7_days']\n",
    "    \n",
    "    if plot==1:\n",
    "        ts[plotColList][\"2016-01-01\":\"2017-05-31\"].plot(grid=True)\n",
    "    \n",
    "    return ts[:][\"2017-04-23\":], errorOccured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records to Process: 829\n",
      "Processing Index: 0 - Store ID: air_00a91d42b08b08d9\n",
      "Processing Index: 300 - Store ID: air_629edf21ea38ac2d\n",
      "Processing Index: 600 - Store ID: air_b8d9e1624baaadc2\n",
      "\n",
      "\n",
      "Finished All Records\n"
     ]
    }
   ],
   "source": [
    "# Get all Unique Visits\n",
    "visitStoreArr = rundf['air_store_id'].unique()\n",
    "\n",
    "orderParam = (7,0,0)\n",
    "visitCorrThreshold = 0.49\n",
    "\n",
    "wkdayList = ['Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday']\n",
    "#exogParam = ['holiday_flg','dayofweek_num']\n",
    "#exogParam = ['holiday_flg'] + wkdayList\n",
    "#exogParam = ['holiday_flg','weekend'] + wkdayList  # Weekend doesn't seem to make a difference when combined.\n",
    "exogParam = ['holiday_flg','weekend'] # combined with holiday, this over-all does better\n",
    "\n",
    "shift=0\n",
    "plot=0\n",
    "\n",
    "# Default to the max of the array\n",
    "maxLoopRun = len(visitStoreArr)\n",
    "\n",
    "# if we are testing\n",
    "testRun = 0\n",
    "if testRun > 0:\n",
    "    maxLoopRun = testRun\n",
    "\n",
    "print('Total Records to Process: ' + str(maxLoopRun))\n",
    "    \n",
    "i=0\n",
    "while i < maxLoopRun: \n",
    "    \n",
    "    #if i%300==0:\n",
    "    if i%300==0:\n",
    "        print('Processing Index: ' + str(i) + ' - Store ID: ' + visitStoreArr[i])\n",
    "    \n",
    "    # Log Start\n",
    "    logger.info('Start Index: ' + str(i) + ' - Restaraunt: ' + str(visitStoreArr[i]))\n",
    "    \n",
    "    # Run TS\n",
    "    #dfTsRun, bError = fn_run_arima_timeseries(visitStoreArr[i], orderList)\n",
    "    #dfTsRun, bError = fn_ts_forecast_ARIMA_exog(dfSuper, visitStoreArr[i], exogParam,orderParam, visitCorrThreshold, shift, plot)\n",
    "    dfTsRun, bError = fn_ts_forecast_ARIMA_exog2(dfSuper, \n",
    "                                                visitStoreArr[i], '7-day-SMA',\n",
    "                                                exogParam,orderParam, visitCorrThreshold, shift, plot)\n",
    "    \n",
    "    '''df, \n",
    "    storeId, colName, \n",
    "    exogParam, orderParam,\n",
    "    visitCorrThreshold, shift, plot'''\n",
    "    \n",
    "    if bError == True:\n",
    "        placeHolder=1\n",
    "        \n",
    "    # TODO:Reformat (possibly done), any numbers\n",
    "    dfTsRun.reset_index(inplace=True)\n",
    "    dfTsRun = dfTsRun[['air_store_id','index','visitors','forecast']]\n",
    "    # colNames = ('visit_date','air_store_id','visitors','visitors_log','forecast','forecast_log','forecast_logExp')\n",
    "    dfTsRun = dfTsRun.rename(columns={'index': 'visit_date'})\n",
    "    \n",
    "    with open(exportResultsSubmissionFileName, 'a') as f:\n",
    "        dfTsRun.to_csv(f, header=includeSubmissionHeaderRunOnce, index=False, quotechar='\"')\n",
    "        f.close()\n",
    "        includeSubmissionHeaderRunOnce = False\n",
    "    \n",
    "    # Log Start\n",
    "    logger.info('Finish Index: ' + str(i) + ' - Restaraunt: ' + str(visitStoreArr[i]))\n",
    "    \n",
    "    # increment\n",
    "    i=i+1\n",
    "    \n",
    "print('\\n')\n",
    "print('Finished All Records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close out\n",
    "logger.info('Stop Logging')\n",
    "handlers = logger.handlers\n",
    "for handler in handlers:\n",
    "    handler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Submission Length\n",
      "forecast\n",
      "Wrote file: D:\\project\\data\\kg_jpn_rest\\export\\20180102_subm_frcst_flt.csv\n"
     ]
    }
   ],
   "source": [
    "hlp.fn_finalize_submission_file('','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
